\documentclass{article}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{bbm}
\usepackage[most]{tcolorbox}
\newcommand{\e}[1]{{\mathbb E}\left[ #1 \right]}
\definecolor{block-gray}{gray}{0.90}
\newtcolorbox{code}{colback=block-gray,grow to right by=-1mm,grow to left by=-1mm,boxrule=0pt,boxsep=0pt,breakable}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\title{\vspace{-3cm}Charlie meeting 20th June\vspace{-3em}}
\author{}
\date{}

\begin{document}
\maketitle
\section*{Meeting}
In this meeting, Andrew and I discussed the result I produced last week. 
Everything is working, which is good. 
This week I will work on different type of paramtisation to paramatise the energy function, Andrew suggested a few different methods as basis functions, such as polynomial, splines and Fourier representation. 
We also talked about my methods of adding jitter, which might not be entire reasonable so I am fixing that too. 
He also talked about a way that makes this methods scalable, which is that instead of a grid of invariance, we only condition on the local area around the data points as well as the test points we wish to evaluate. 
If noise too small or too much data, it's not good for nonlinear nor damping.
But in most reasonable cases, 30 seconds with 30 points with noise 0.1 the kernel all worked well.
We have four scenarios to check.
Noisy being 0.1 and not noisy being 0.01. Lots of data means 0.1 time spacing.
left is naive, right is invariance
\begin{enumerate}
  \item lots of data, noisy
  \item little data, noisy
  \item lots of data, not noisy
  \item little, not noisy
\end{enumerate}
\begin{flushleft}
\resizebox{\columnwidth}{!}{\begin{tabular}{ r|r|r|r|r} 
 & lots of data, noisy& little data, noisy & lots of data, not noisy & little data, not noisy\\
 \hline
 SHM &-1323,
-1305 &65,
81 &1354,
1394 & 271,
311\\
 \hline
Fixed Damped SHM & -1310,
-1292; -1305,
-1290; -1310,
-1297&78,
96; 79,
92; 54,
69& 1387,
1419; 1349,
1338; 1404,
1346 & 289,
322; 269,
254; 329,
253\\
 \hline
Dynamical Damped SHM & -1310,
-1292; -1305,
-1289; -1310,
-1295 &78,
95; 79,
96; 54,
68 &1387,
1423; 1349,
1382; 1404,
1435 &289,
316; 269,
258; 329,
256\\
 \hline
Pendulum & -1307,
-1288 &61,
65 &1403,
1451 & 232,
115 \\
 \hline
Fixed Damped Pendulum & -1257,
-1239; -1324,
-1308; -1315,
-1301& 52,
62; 55,
69; 67,
79&1397,
1428; 1401,
1363; 1373,
1266 & 272,
133; 267,
179; 296,
175 \\
 \hline
Dynamical Damped Pendulum &-1257,
-1239; -1324,
-1308; -1315,
-1299 & 52,
62; 55,
70; 67,
82  &1397,
1442; 1401,
1442; 1373,
1412 & 272,
135; 267,
209; 296,
241\\
 \hline
\end{tabular}}
\end{flushleft}
We can see that most of the entries are good, except lots of data not noisy for fixed damping when damping is high, but dynamical damping still outperforms.
A lot of problem is found in low data, low noise area; especially when it is non linear or damping is high. 
This make sense in some way.
For the first case, since it is harder to for fixed damping to handle and predict larger damping, if there are less noise to smooth out the effect of damping, so that damping is more varying, it makes sense for fixed damping to perform worse. 
For the second case, I think the main reason is that the estimation of acceleration and velocity is already not so good to start with, if we have low noise, it's like we are assuming that's the truth.
Therefore, our physical intuition (invariance) will misguide us since we will be expecting something else. 
\section*{Paramatisation}
\section*{Local Invariance}
Before we have
$$
\begin{pmatrix}
  \mathbf{f}(X_1)\\\mathbf{f}(X_2) \\ L_{X_g}[\mathbf{f}(X_g)]
\end{pmatrix}
\sim \mathcal{N}
\left(
\begin{pmatrix}
  \mathbf{0}_{2n} \\ \mathbf{0}_{2m} \\ \mathbf{0}_{\ell}
\end{pmatrix},
\begin{pmatrix}
K(X_1, X_1) & K(X_1, X_2) & B_1 \\
K(X_2, X_1) & K(X_2, X_2) & B_2 \\
C_1         & C_2         & D \\
\end{pmatrix}
\right), 
$$
Before with the grid of invariance $X_g$ is fixed, now they are consist of the local area around the test points and the training trajectory data. 
Therefore, the only change we need to make is $X_g$ is now a function of $X_1, X_2$ so that $X_g = X_g(X_1, X_2)$.
As a result, now $B, C, D$ will all be a function of $X_1, X_2.$
Essentially, we just have to redefine our grids to be the neighbourhoods around test and training points.  
In our case, before we have 1600 points in a 6 by 6 area, now perhaps we can have 4 points around the 0.1 by 0.1 area around each point. 

\section*{Change to damping approximate invariance}
Now instead of our original method of conditioning on an either analytical form or like a fix or random vector, we will allow the invariance to be noisy.

Before we had, 
$$
\begin{pmatrix}
  \mathbf{f}(X) \\ L_{X_g}[\mathbf{f}(X_g)]
\end{pmatrix}
\sim \mathcal{N}
\left(
\begin{pmatrix}
  \mathbf{0}_{2n} \\ \mathbf{0}_{\ell}
\end{pmatrix},
\begin{pmatrix}
A & B \\C&D\\
\end{pmatrix}
\right), 
$$

Now it is like when we have noise on the observation, we also have noise on the invariance. 
Therefore, in a simliar way to $y=f+\text{noise}_{\sigma^2_n}$, wer have $\tilde{L} = L+\text{noise}_{\epsilon}$. 
Since all the noise are uncorrelated (Gaussian white noise), we will only modify the D matrix (signal noise will be accounted for by GPflow).
Therefore, we will have $\tilde{D}=D+\epsilon I$.
So we have 
$$
\mathbf{f}(X) | \tilde{L}_{X_g}[\mathbf{f}(X_g)]=0 \sim \mathcal{N} \left(0, A-B\tilde{D}^{-1}C\right).
$$
The rest would be the same as before. 

\section*{Question}
I still don't quite understand the difference between conditioning on a random vector and add noise to the thing itself.
\end{document}

