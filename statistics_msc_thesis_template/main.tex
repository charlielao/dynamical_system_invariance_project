\documentclass{statsmsc}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{nccmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{physics}
\usepackage{tikz}
\usepackage[outline]{contour} % glow around text
\usepackage{bbm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcommand{\e}[1]{{\mathbb E}\left[ #1 \right]}
\definecolor{block-gray}{gray}{0.90}
\newtcolorbox{code}{colback=block-gray,grow to right by=-1mm,grow to left by=-1mm,boxrule=0pt,boxsep=0pt,breakable}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\usetikzlibrary{patterns,snakes}
\usetikzlibrary{arrows.meta} % for arrow size
\contourlength{0.4pt}

\colorlet{xcol}{blue!70!black}
\colorlet{darkblue}{blue!40!black}
\colorlet{myred}{red!65!black}
\tikzstyle{mydashed}=[xcol,dashed,line width=0.25,dash pattern=on 2.2pt off 2.2pt]
\tikzstyle{axis}=[->,thick] %line width=0.6
\tikzstyle{ell}=[{Latex[length=3.3,width=2.2]}-{Latex[length=3.3,width=2.2]},line width=0.3]
\tikzstyle{dx}=[-{Latex[length=3.3,width=2.2]},darkblue,line width=0.3]
\tikzstyle{ground}=[preaction={fill,top color=black!10,bottom color=black!5,shading angle=20},
                    fill,pattern=north east lines,draw=none,minimum width=0.3,minimum height=0.6]
\tikzstyle{mass}=[line width=0.6,red!30!black,fill=red!40!black!10,rounded corners=1,
                  top color=red!40!black!20,bottom color=red!40!black!10,shading angle=20]
\tikzstyle{spring}=[line width=0.8,blue!7!black!80,snake=coil,segment amplitude=5,segment length=5,line cap=round]
\tikzset{>=latex} % for LaTeX arrow head
\tikzstyle{force}=[->,myred,very thick,line cap=round]
\def\tick#1#2{\draw[thick] (#1)++(#2:0.1) --++ (#2-180:0.2)}
\newsavebox{\overlongequation}
\newenvironment{CentreLongEquation}
 {\begin{displaymath}\begin{lrbox}{\overlongequation}$\displaystyle}
 {$\end{lrbox}\makebox[0pt]{\usebox{\overlongequation}}\end{displaymath}}


\title{Learning Invariances in Dynamical System}
\author{Cheng-Cheng Lao}
\CID{01353756}
\supervisor{Dr. Andrew Duncan and Dr. Mark van der Wilk}
\date{\today}
%For today's date, use:
%\date{\today}
\logoimg{}


% THIS IS WHERE NEW COMMANDS CAN BE DEFINED
% commands below only used in the proof; otherwise can be deleted
\newcommand{\consta}{a}
\newcommand{\X}{X}
\newcommand{\EE}[1]{ \mathrm{E} [ #1 ] }
\newcommand{\inparenth}[1]{\left( #1 \right)}

\begin{document}

% Generates the Title Page
\maketitle


% Generates plagiarism declaration
\declarationname{Cheng-Cheng Lao}
\declarationdate{\today}
\declaration 


\begin{abstract}
Predicting the evolution of dynamical systems reliably has important practical values as they are used to model lots of natural and engineered processes across biology, physics and engineering.
To improve the performance of model in terms of predictive performance and data efficieny, we can embed inductive bias, which is some prior knowledge we have, into our statistical models; in physics, the idea of symmetry and invariance are very powerful inductive bias to build into models.
Examples of such invariance includes conservation of energy or conservation of momentum.
\cite{Mark2017} has shown marginal likelihood is a good objective function to uncover the underlying invariance and symmetry of the data, which was affine transformation of images in their case. 
In this report, we extend their work and propose a Gaussian Process model that is able to capture the invariance in a dynamical system and hence recover the law of physics automatically.
We also show the discovered invariance as inductive bias greatly improved the data efficiency and predictive performance, which is tested on a variety of systems, including one and two-dimensional simple harmonic motion, nonlinear pendulum as well as double pendulum.
We have also demonstrated that the model is robust even when the system is not ideal and has dissipative effect where the invariance no longer holds exactly. 
We are even able to perform inference on this latent dissipative effect.
\end{abstract}

\begin{acknowledgements}
I would like to thank Dr Andrew Duncan and Dr Mark van der Wilk for being amazing supervisors, who provide guidance that I cannot complete the project without, and making time even though they are incredbily busy. 
I would also like the thank my family and friends for their support.

\end{acknowledgements}

% add glossary?

% VERY IMPORTANT
% This command switches from Roman to Arabic numbering for main part of thesis
\mainmatter
\tableofcontents

\chapter{Introduction}
Dynamical systems are one of the richest theory that models the real world with wide application across all science and engineering and therefore it is very valuable to be able to reliably predict the evolution of a system for practical purposes.
Recently, data intensive machine learning methods, such as deep learning, has shown some very powerful results. 
However, the amount of data required to achieve reasonable performance are often enormous, will often involve lots of manual labour to label and process the data.
Sometimes, the data could be very difficult to come by in biological and medical settings.
Therefore, data efficiency is the key if we wish to study more complicated systems.
Inductive bias, an intitial choice of hypothesis space (\cite{Baxter2000}), is one way to achieve this goal.
Researchers have embeded the inductive bias into the model in different ways so that the model will possess some prior knowledge about either the task or data before observing the data.
Assuming the inductive bias is correct, we would expect the model to perform better, either in a data efficiency or prediction performance way.

One very powerful inductive bias is the use of symmetry and invariances, that puts strong constraints on the model. 
It is the basis of many modern powerful machine learning models, such as Convolution Neural Network (CNN), utilising the translational symmetry in natural images.
To illustrate, imagine in a binary classification task of dogs and cats, we need to distinguish a photo of a dog versus a cat. 
The structure of CNN is designed such that the \textbf{same} filter, which can be thought of some sorts of detector, is scanned through different parts of the image.  
As a result of that, it does not matter where the dog is located in the photo; let it be top right corner or bottom left corner, the same filter will be able to identify the dog either way.
This greatly simplies the learning of the model.
This is in contrast to feeding the data into a simple full connected neural network, where the information of where the dog is located will also be learnt and making the learning much less efficient and effective, and also redundant.
We can therefore see how a correct inductive bias can massively simplify a problem.

Nevertheless, in many of these models, the inductive bias is often built in to the structure by construction, which would require prior human knowledge.
In order to learn inductive bias automatically, we need an objective function that encourages the model to discover the underlying symmetry and invariance underlying the data. 
\cite{Mark2017} as well as Van der Ouderaa and Van der Wilk (2022) has demonstrated that using marginal likelihood as the objective is useful in learning the invariance, which is affine transformation of images in their case.
Intuitively, marginal likelihood, $p(Y)=\int p(Y|X)p(X)dX$ ($Y$ is the target and $X$ is the input) measures how well the data is explained by the model.
This metric is common in Bayesian model selection and in our case: identifying the correct invariance. 
Gaussian Process (GP) was particularly useful for this purpose thanks to its ability to estimate marginal likelihood exactly, and we can embed the inductive bias in the kernel function of the GP (see chapter 2).
In this project, we aim to extend \cite{Mark2017} to learn invariance in dynamical systems to improve the data efficiency of system evolution prediction and recover the physical invariance underlying the data automatically. 

In the rest of the report, we will cover the essential theortical background to understand GP, dynamical systems, as well as symmetry and invariances in chapter 2. We will also review related work in this section.
In chapter 3, we will derive the theoretical apsects and the constructions of the invariance kernels in dynamical systems.
Following that, we illustrate the implemented results on various dynamical systems in chapter 4 along with discussions.
Finally, we conclude in chapter 5 and discuss future works.

 
\chapter{Background}
In this chapter, we will cover the background knowledge required to understand the remaining report, including Gaussian Process, dynamical systems as well as symmetry and invariances. 
This section will also cover related work in the field.

\section{Gaussian Process}
Gaussian Process (GP) can be thought of a distribution over functions \citep{rasmussen_williams_2006}.
More formally, \begin{quote}GP is a collection of random variables, any finite number of which have a joint Gaussian distribution\end{quote}
We will be able to specify a GP prior on $f(\mathbf{x})$ by mean function $m(\mathbf{x})=\mathbb{E}[f(\mathbf{x})]$ as well as the kernel function $K(\mathbf{x}, \mathbf{x'})=\mathbb{E}[f(\mathbf{x})-m(\mathbf{x})(f(\mathbf{x'})-m(\mathbf{x'}))]$, which is the covariance function.
We then then write
$$
f(\mathbf{x})\sim\mathcal{GP}(m(\mathbf{x}),K(\mathbf{x},\mathbf{x'})).
$$
Without loss of generality, we will now assume $m(\mathbf{x})=\mathbf{0}$, a common zero mean prior.
In a noise free scenario, where we observe $f$ directly, we will have
$$
\begin{bmatrix}
  f\\f^*
\end{bmatrix}
\sim \mathcal{N}
\left(\mathbf{0}, \begin{bmatrix}K(X, X) & K(X, X^*) \\ K(X^*, X) & K(X^*, X^*) \end{bmatrix}\right),
$$
where we have $X, f$ being the training input and target, with $X^*, f^*$ are the testing input and targets.
If we assume the signal is not noise free such that we assume an additive Gaussian noise on random variable $f$, $y=f+\epsilon$, where $\epsilon\sim\mathcal{N}(0, \sigma_n^2)$.
Since the noise is a white noise and are uncorrelated with each other, we will only need to modify the variance part of the signal and not its covariance with other latent variables $f^*$,
\begin{equation}
\begin{bmatrix}
  y\\f^*
\end{bmatrix}
\sim \mathcal{N}
\left(\mathbf{0}, \begin{bmatrix}K(X, X)+\sigma^2_n\mathbb{I} & K(X, X^*) \\ K(X^*, X) & K(X^*, X^*) \end{bmatrix}\right).
\label{equ:joint_gp_noise}
\end{equation}

An important identity we use throughout the thesis is the Gaussian conditional formula: if
$$
  \begin{bmatrix}
    x\\y
  \end{bmatrix} 
  \sim \mathcal{N}
  \left(
    \begin{bmatrix}
      \mathbf{\mu}_x\\
      \mathbf{\mu}_y
    \end{bmatrix},
    \begin{bmatrix}
      A & C \\
      C^T & B\\
    \end{bmatrix}
  \right), 
$$ 
then 
\begin{equation}
  x|y\sim \mathcal{N}\left(\mathbf{\mu}_x+CB^{-1}(y-\mathbf{\mu}_y),A-CB^{-1}C^T\right)
  \label{equ:normal_condtion}
\end{equation}
We can therefore predict our new points by conditioning on our data, obtaining: 
\begin{equation*}
\begin{gathered}
f^*|X, y, X^* \sim \mathcal{N} \left(\overline{f^*}, \mathrm{cov}(f^*)\right)\\ 
\overline{f^*} = K(X^*, X)[K(X,X)+\sigma^2_n\mathbb{I}]^{-1}y\\
\mathrm{cov}(f^*) = K(X^*, X^*) - K(X^*, X)[K(X, X)+\sigma^2_n\mathbb{I}]^{-1}K(X, X^*)
\end{gathered}
\end{equation*}

We also have the log marginal likelihood given by 
$$
\log p(\mathbf{y} \mid X)=-\frac{1}{2} \mathbf{y}^{\top}\left(K+\sigma_{n}^{2} \mathbf{I}\right)^{-1} \mathbf{y}-\frac{1}{2} \log \left|K+\sigma_{n}^{2} \mathbf{I}\right|-\frac{n}{2} \log 2 \pi.
$$

Below we demonstrated an example adapted from \cite{GPflow2017} to showcase the fitting procedure of a GP.
Different kernels defined by their various functional form as below imposes different prior knowledge on the GP.
For instance, one of the most commonly used kernel, RBF, requires smoothness; Matérn requires once differentiability; Periodic kernel requires the prior to have periodic structure.
Below in figure \ref{fig:prior}, we showed 5 samples drawn from GP prior from each of the kernel, with mean zero.
\begin{equation*}
  \begin{gathered}
    k_{RBF}(r) = \exp(-\frac{r^2}{2l^2}) \\
    k_{\text {Matérn}}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2 \nu} r}{\ell}\right)^{\nu} K_{\nu}\left(\frac{\sqrt{2 \nu} r}{\ell}\right) \\
    k_{periodic\ RBF}(r)=\exp \left(-\frac{2 \sin ^{2}\left(\frac{r}{2}\right)}{\ell^{2}}\right),
  \end{gathered}
\end{equation*}
where $r=|x-x'|$, the distance between two input.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../figures/prior.pdf}
  \centering
  \caption{Samples from different GP priors of RBF, Matérn and periodic kernel}
  \label{fig:prior}
\end{figure}

Now we intend to fit a GP to a dataset where the underlying function is $y=(x+x^2)\sin(x)$, arbitrarily defined for demonstration purpose.
We randomly sampled five points and the fit is as shown below in figure \ref{fig:posterior}.
We also sampled 10 posteriors.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../figures/posterior.pdf}
  \centering
  \caption{GP fit of the function $f=(x+x^2)\sin(x)$ with posterior samples, light shaded blue indicates 95\% credible interval}
  \label{fig:posterior}
\end{figure}

We can see the function has low uncertainity around the data points and fall back to the GP prior (RBF in this case) away from the data.

\subsection{GPflow}
While we can in principle build our own GP objects using linear algebra and statistical libraries, such as Numpy, Scipy, from scratch, it is better to avoid errors and improve development speed by using a well-tested, robust and efficient library called GPflow, which will be used throughout the project (\cite{GPflow2017})\footnote[1]{The codes can be found in \url{https://github.com/charlielao/dynamical\_system\_invariance\_project}}.
It is a Python library built on TensorFlow to allow efficient and fast computation on GPUs to do GP inferences.
A particular powerful feature is that it allows atuomatic tuning of hyperparameter such as lengthscales and variance of a kernel by computing the gradient (using automatic differentiation feature of Tensorflow) and perform optimisation.
In later sections, when we will need to optimise more parameters than aforementioned ones with respect to maginal likelihood objective, the library will be proved to be very useful.

\section{Dynamical Systems}
Dynamical systems describe the evolution of a system in time, and can be broadly classified to two catergories: differential equations and difference equations, with the former being continous time and the latter being discrete time (\cite{strogatz_2019}). 
Here, we will illustrate the theory and notation using the continous time (differential equation) description.
There are many real world systems that can be modelled by a dynamical system; for example a huge class of problems in engineering and physics or biological processes. 
A simple example, which will also be explored later, is a damped harmonic oscillator, with equation of motion
\begin{equation}
m\frac{d^2x}{dt^2}+b\frac{dx}{dt}+kx=0,
\label{equ:damped}
\end{equation}
where $m, k, b$ are constants that depend on the setup and $x$ is the displacement of the mass.
In general an $n^{th}$ degree differential equations can be written as 
$$
\frac{d^{n} x}{d t^{n}}=F\left(t, x, \frac{d x}{d t}, \ldots, \frac{d^{n-1} x}{d t^{n-1}}\right),
$$
where $F$ is just some function (\cite{glendinning_1994}) and we will need $n$ initial conditions to specify a solution for the differential equations.
We can also write $n^{th}$ degree ordinary differential equations (ODEs) as $n$ degree one coupled ODEs \citep{strogatz_2019}. 
For example, we can rewrite equation \ref{equ:damped} as 
$$
\begin{cases}
  \frac{dx}{dt} = v\\
  \frac{dv}{dt} = \frac{-bv-kx}{m}\\
\end{cases}
$$
More generally, we can write 
$$
\begin{cases}
  \dot{x}_1=f_1(x_1, \dots, x_n)\\ 
  \vdots\\
  \dot{x}_n=f_n(x_1, \dots, x_n)\\ 
\end{cases}
$$

We can then visualise the state of the system specified by the $n$ variables, $x_1 \dots x_n$ in the phase space, and this system will evolve by moving in the phase space as time progresses. 
For instance, the damped oscillator is of degree 2 so there will be two variables describing its evolution, which in this case will be $x$ and $\frac{dx}{dt}=\dot{x}$, which is the displacement as well as velocity.
Another example would be a naive model describing exponential growth of a organism, such as bacteria population in a petri dish
$$
\dot{x}=rx,
$$
and this time it is one-dimensional with the phase variable being the population number, with $r$ again being a constant.
Another important concept is linearity and nonlinearity, which is referring to the $f_1, \dots ,f_n$ above.
If they depend on $x_1 \dots x_n$ linearly, then the system is linear, else it would be nonlinear.
Other examples of dynamical systems include predator-prey dynamics, planetry evolution, and many biological processes, such as drug treatments.
The systems discussed in the report include simple harmonic oscillators and single as well as double pendulums.



\section{Symmetry and Invariances}
Invariances are functions of the phase variables of a dynamical system that are unchanged throughout the evolution of the system over time. 
An example would be from physics, the conservation of energy, that energy will remain unchanged or constant throught out the trajectories of the system.
This is related to the symmetry under time translation.
There are other types of symmetry, such as translational symmetry, rotational symmetry, permutation symmetry, which are widely explored in geometric deep learning literature \cite{Bronstein2017}. 
More formally, we can describe symmetry using a little group theory.
Refering to \cite{Kondor2008}, we have symmetries and associated invariance if we have a symmetry group $G$ acting on $\mathcal{X}$, the input space, by $x\mapsto T_g(x)$ for $g\in G$, where $T$ is some transformation function parameterised by $g$. 
Then we should have $f(T_g(x))=f(x)$ for any function $f$, in other words, invariant under symmetry transformation.
For instance, if it is a rotational symmetric group then we will have $T$ corresponds to the rotation transformation of angle $g$, and we expect $f$ to be invariant under rotation.
In dynamical system, it has a slightly different interpretation.
From \cite{marsden_sirovich_antman_2008}, in this description of dynamical system, we consider a generator $\mathcal{L}$, which is an operator that generates the evolution of some function quantity that depends on the phase variables over time.
If we have the ODE $\frac{dx}{dt}=f$; then \begin{equation}\mathcal{L}[E]=\frac{dE}{dt}=f(x)\cdot \nabla_x E(x),\label{equ:LE}\end{equation} followed by simple chain rule, where $E$ is some function of the phase.
If we would like $E$ to be the conserved invariant quantity, we would then require $\mathcal{L}[E]=0$. 
This will then define what we mean by invariance in the context of dynamical system: some quantity depending on the input that does not change along the evolution of the system.

At first sight, there seems no apparent symmetry involved that is obvious like the rotational symmetry case. 
Neverthless, in physics, there is a deeper connection between symmetry and invariance, namely the Noether's Theorem, which is rather involved in theoretical classical physics and interested readers can refer to \cite{lemos_2018}.
In plain words, every conservation law (invariance) is associated with an underlying symmetry.
For instance, conservation of momentum is obeyed when there is a spatial translation symmetry of the system. 
Similarly, rotational symmetry is associated with the conservation of angular momentum.
There are more abstract conservation law, such as conservation of electrical charge is related to Gauge symmetry.
Here the more relevant ones is the conservation of energy, which is related to the symmetry of the system under temporal translation, i.e. the quantity is unchanged as time passes.


\section{Related Work}
There are many existing literature in the field of learning invariance, prediction in dynamical systems, as well as physics based machine learning methods.
A summary of comparison is made in table \ref{tab:compare}.

\subsection{Physics Informed Machine Learning}
Many physics informed machine learning techniques exist as outlined in \cite{Cuomo2022, PIML}
For example, in Physics Informed Neural Network (PINN) from \cite{Raissi2019}, the authors embedded the functional form of partial differential equations (PDEs) that describe the physics of the problem into the loss function so that the neural network model assume the PDE; they also allow the parameters in the PDEs to be learnt. 
There are many variants of such as \cite{Qian2020}.
In \cite{Raissi2018}, it even uses a GP to model the evolution of the state by using the PDE form as a linear operator to evolve the GP; since the linear transform of GP is still a GP, we will get joint GP, then we can do inference.
However, they are restricted to known PDE expression and only learn the coefficients instead of learning the whole physics from scratch. 
Moreover, for nonlinear dynamics, it has to use a linear approximation for the method to work, which is very restrictive.
There are also versions of PINN that takes conservation of energy into account such as \cite{Jagtap2020}.
However, it still suffers from other problems described above.

\subsubsection{Energy Conserving Neural Networks}
There are other approaches like Hamiltonian Neural Network (HNN) \citep{HNN}, which enforces the law of Hamiltonian mechanics (\cite{lemos_2018}), a model in theortical physics, on the neural network.
As a result, HNN automatically conserves the Hamiltonian or almost equivalently, the energy.
The experiments and setup of this report roughly follows that of the HNN paper.
There are also variants originated from this, such as the more general Lagrangian Neural Network \citep{Cranmer2020} using another theoretical physics framework or other energy conserving nerual networks described in \cite{Zhong2021}.
However, due to their strong invariance enforcing nature, they often do not do very well with dissipative systems; 
\cite{DissipativeHNN} models the dissipative part explicitly resolves this problem.
Nevertheless, these systems can only work on a very specific types of dynamical system, namely the ones that follow Sympletic form, or more simply follows the Hamiltonian mechanics, which is only a small subset of all dynamical systems.
There are a lot more general dynamical systems that has some sort of invariance property that is not able to utilise these energy conserving neural networks.
Moreover, learnability of invariance is less involved here that they are able to learn the value of energy but not actually recover the form of physics. 

\subsubsection{Symbolic approach}
An alternative way to recover the law of physics from trajectory uses various forms of symbolic regression, such as \cite{Udrescu2020, Papastamatiou2022} or some form of graphical representation \cite{GraphSymbolicPhysics, flexiblePhysics}.
In \cite{Liu2021}, it even able to deduce functional form the conserved quantity (along with its symbolic form) from trajectory.
However, again these are very physics focused examples, and the symbolic representation is often involves brute force; but of course it improves interpretability over functional approximation solution.

\subsubsection{ODE approach}
We can also directly works on the ODE that describes the physics of the system not dissimliar to PINN. 
Neural ODE (\cite{NeuralODE}) is a great example of this; 
however, again it does not use any knowledge of the invariances and symmetries.
In \cite{Lim2022}, they claimed that while Neural ODE is flexible, without built-in inductive biases of energy conservation, the model tends to grow or shrink in energy over a long period of time.
Furthermore, due to the extremely flexible expressibility of neural network, the ODE it learnt may not be the true physical model; which demonstrates the importance of building in such inductive bias.  

\subsection{Symmetry and Invariance in Machine Learning}

\subsubsection{Learning Invariance using Marginal Likelihood}
This paper forms the basis of this project.
In \cite{Mark2017}, the authors propose learning the form of invariances of the problem by parameterising the symmetry and optimise them with respect to the marginal liklihood.
The main problem studied in the paper is the MNIST dataset, and that the symmetry under consideration is all form of affine transformation of the images, such as rotation, shear, translation; and the invariance in this case is the label of the digit. 
It suggests embedding the invariance in the model (GP in this case) as an alternative to data augementation.
The authors created a special kind of kernel for GP that respects the symmetry of the problem.
The results are very good compare to the baseline RBF kernel, and it also is able to recover the true paramertised invariance. 
The logical flow and procedure of investigation of this report follows closely on this paper. 
A related paper following that uses a neural network instead of GP is Van der Ouderaa and Van der Wilk (2022) using the same idea by optimising the marginal likelihood.

\subsubsection{Geometric Deep Learning}
Geometric deep learning proposed by \cite{Bronstein2017}, is a unifying framework that aims to explain the success of several popular neural network architecture. 
For example, Convolution Neural Network uses the translation symmetry embedded in natural images. 
Another example is Graph Neural Network utilises the permutation symmetry that the graphs is invariant under permutation.
Following this framework, multiple papers are proposed that aims to exploit further symmetry groups (5Gs Grids, Groups, Graphs, Geodesics, and Gauges).
These show that how powerful the inductive bias of symmetry and invariances are.

\subsection{GP in dynamical systems}
There are also a number of research work that use GP to predict dynamical system evolution.
One of them is the aforementioned \cite{Raissi2018}.
There is also \cite{Rath2020}, that utilise the Sympletic nature of Hamiltonian systems to directly predict the flowmaps of the system, which is similar to HNN.
There is also \cite{Heinonen2018}, that also uses GP to model the ODE models in a nonparametric manner; but again it does not use invariance, and is more like Neural ODE. 

\begin{table}[H]
    \centering
\begin{tabularx}{\linewidth}{c|LLL} 
    \hline
 Methods & Respect the physics laws & Recover the physics laws & Generalise to dynamical systems beyond physics  \\ 
    \hline
  ODE approach & X & O & O \\
  Symbolic approach & O & O & X \\
  Physics informed ML & O & X & X \\
  Energy conserving NN & O & X & X \\
  GP in dynamical system & X & O & O\\
  Our method & O & O & O\\
    \hline
\end{tabularx}
\caption{Comparing the capabilities of different existing approach to learning invariance in dynamical systems}
\label{tab:compare}
    \end{table}



\chapter{Invariance Kernel}
In this chapter we will start with general theoretical construction of invariance kernel given the knowledge of the invariance of the system.
We will then describe how we deal with dissiaptive systems.
Finally, we will demonstrate how we are able to learn invariances without any prior knowledge of the system.

\section{General Construction}
If we are given a dynamical system of dimension $d$ with variables $\mathbf{q}, \mathbf{p}$, where $\mathbf{q} = (q_1, q_2,\dots,q_d)$ is the vector of positional coordinates, and $\mathbf{p}=(p_1, p_2, \dots, p_d)$ is the vector of velocity coordinates of the states of the dynamical system.
We are interested to predict the future trajectories of the state of the system.
Therefore, we would like to know the time derivative of these coordinates so we can update them using an integrator (e.g. Euler, Runge-Kutta). 
These time derivatives are referred to as the dynamics of a dynamical system, which governs the evolution of the dynamical system, and is a function of the coordinates $\mathbf{q}$ and $\mathbf{p}$ (it could be explicitly time dependent but it will be a straightforward generalisation, and is not considered here). 
For our systems, we will denote the dynamics of $\mathbf{p}$ as $\frac{d\mathbf{p}}{dt}=\mathbf{a}(\mathbf{q}, \mathbf{p})$, and that of $\mathbf{q}$ as $\frac{d\mathbf{q}}{dt}=\mathbf{v}(\mathbf{q}, \mathbf{p})$.
Astute readers or someone with physics background would notice $p$ is related to the velocity, which in principle should be exactly equal to $v$; while this is true in physical dynamical system, the setup is generally applicable to non physical dynamical systems too, so that we will not use this knowledge. 
Once we have the dynamics, we can integrate up to obtain the future trajectories. 
Notation wise, we will collect the two dynamics term and call them $$\mathbf{f(\mathbf{q}, \mathbf{p})}=\begin{pmatrix}
    \mathbf{a(\mathbf{q}, \mathbf{p})}\\\mathbf{v(\mathbf{q}, \mathbf{p})}
\end{pmatrix}$$
For simplicity of notation, the dependence on $\mathbf{q}, \mathbf{p}$ is now implicit. 
We will put independent GP prior on $\mathbf{a}$ and $\mathbf{v}$ since there are no reason we should assume they are correlated. 
For the choice of kernel, we chose the standard smooth kernel, the squared exponential kernel, or RBF, and we denote this kernel to be $K_{RBF}$. 
We will also denote any set of points in the input space of length $n$ and dimension $d$ as $$X\equiv\begin{pmatrix}
    \mathbf{x}_1\\\mathbf{x}_2\\\vdots\\\mathbf{x}_n
\end{pmatrix}=\begin{pmatrix}
    q_{11}& q_{21} &\dots& q_{d1} & p_{11} & p_{21} & \dots & p_{d1} \\
    q_{12}& q_{22} &\dots& q_{d2} & p_{12} & p_{22} & \dots & p_{d2} \\
    \vdots &\vdots &\ddots &\vdots &\vdots &\vdots &\ddots &\vdots \\
    q_{1n}& q_{2n} &\dots& q_{dn} & p_{1n} & p_{2n} & \dots & p_{dn} \\
\end{pmatrix}.$$
Without loss of generality, we will choose to stack the dynamics vertically in $\mathbf{f}$; for example in a $d$-dimensional system, we have $$\mathbf{f}(X)=\begin{pmatrix}
    a_1(\mathbf{x}_1)\\
    \vdots\\
    a_1(\mathbf{x}_n)\\
    \vdots\\
    a_d(\mathbf{x}_1)\\
    \vdots\\
    a_d(\mathbf{x}_n)\\
    v_1(\mathbf{x}_1)\\
    \vdots\\
    v_1(\mathbf{x}_n)\\
    \vdots\\
    v_d(\mathbf{x}_1)\\
    \vdots\\
    v_d(\mathbf{x}_n)\\
\end{pmatrix}$$
For our naive independent RBF GP prior, we have block diagonal 
\begin{CentreLongEquation}
$$
K=\mathrm{Cov}(\mathbf{f}(X), \mathbf{f}(X'))=\begin{pmatrix}
    K_{RBF,a_1}(X,X') & \dots  & \dots          & \dots         & 0\\
    \vdots        & \ddots & \dots          & \ddots        & \vdots\\
    0             & \dots  & K_{RBF,a_d}(X,X')  & \dots         & 0 \\
    \vdots        & \ddots & \vdots         & K_{RBF,v_1}(X,X') & \vdots \\
    0             & \dots  & 0              & \dots         & K_{RBF,v_d}(X,X')\\
\end{pmatrix},
$$
\end{CentreLongEquation}
with all the off diagonal terms being zero block matrix because of the independence prior assumption.
This naive kernel will be our baseline to be compared to throughout the whole project.
Now we can start considering the invariance. 
If we assume the invariance constraints has to hold throughout the input space $\mathbb{R}^{2d}$, then we can condition the GP on the a grid of points, which we referred to invariance points, $X_L$, and assume there are $\ell$ of them, and we will call the invariance constraints $\mathcal{L}[E]$ as shown in section 2.3.

The form of $\mathcal{L}[E]$ will depend on the system as well as $X_\mathcal{L}$, and examples will be given in the following sections to make it clear.
Since the conservation of energy, $E$, is one of the most common invariance in physical systems, we will focus on making use of this constraint.
Therefore, we have $E=\text{energy}.$
We can see the invariance constraints will always be a linear function on the dynamics $\mathbf{f}=\begin{pmatrix}
  \mathbf{a} \\ \mathbf{v}
\end{pmatrix}$ almost by definition because of chain rule as shown in the next paragraph.

We have $\text{energy}=E(\mathbf{\theta}, \mathbf{p}, \mathbf{q})$, where $\mathbf{\theta}$ are constants that parameterised the energy function, like mass or gravity.
Then refering back to equation \ref{equ:LE}, we have 
\begin{equation}
\mathcal{L}[E] = \frac{dE}{dt}=\sum_{i=1}^d \frac{\partial E}{\partial p_i} \frac{\partial p_i}{\partial t}+ \sum_{i=1}^d\frac{\partial E}{\partial q_i}\frac{\partial q_i}{\partial t} =\sum_{i=1}^d \frac{\partial E}{\partial p_i} a_i(\mathbf{p,q}) + \sum_{i=1}^d\frac{\partial E}{\partial q_i} v_i(\mathbf{p,q})
\label{equ:invariance_equation}
\end{equation}
We can therefore see it is always a linear combination of dynamics with coefficients independent of dynamics.
As a result, $\mathcal{L}[E]$ can be seen to be a linear transformation of $\mathbf{f}$, and now we call this transformation operator $L$, which will be a function of $\mathbf{p}, \mathbf{q}$ and $\mathbf{\theta}$.
If we apply this linear transformation $L$ on $\mathbf{f}(X_L)$, $L(\mathbf{f}(X_L))$ will again be a GP with a transformed kernel.
Therefore, we have 
\begin{equation}
\begin{pmatrix}
\mathbf{f}(X)\\L[\mathbf{f}(X_L)]
\end{pmatrix}
\sim\mathcal{N}
\left(\begin{pmatrix}\mathbf{0}_{2nd}\\\mathbf{0}_{\ell}\end{pmatrix}, \begin{pmatrix}
    K & LK \\
    KL^T & LKL^T\\
\end{pmatrix}\right)
\label{equ:invariance_joint}  
\end{equation}
To impose the invariance constraints, we then condition the first row on the second row using equation \ref{equ:normal_condtion} to obtain
\begin{equation}
  \mathbf{f}(X)|L[\mathbf{f}(X_L)]=0 \sim \mathcal{N} \left(\mathbf{0}_{2nd}, \begin{pmatrix}
    K-LK(LKL^T)^{-1}KL^T
  \end{pmatrix}\right)
  \label{equ:invariance_condition}
\end{equation}

\section{Damped System}
Up to now, we have considered a perfect system, i.e. ideal world without frictions and the conservation of energy is completely obeyed such that $L=0$ exactly.
However, in a real system, there will be dissipation where energy is lost in the form of heat etc.  
For example, in equation \ref{equ:damped}, the frictional force enters through the $b\frac{dx}{dt}$ term that makes the system loss energy to the surrounding as heat.  
Therefore, to model that, we need to allow "approximate" invariance, such that the invariance $L$ is noisy and not always equal to zero, i.e. soft invariance.
To account for that, in a similar spirit to when we add noise to the underlying latent GP to represent noise signal in equation, where we have $K+\sigma_n^2 \mathbb{I}$ in equation \ref{equ:joint_gp_noise}.
We will therefore add a noise to the invariance evaluation so that we have $L[\mathbf{f}(X_L)]=\epsilon$, where $\epsilon\sim\mathcal{N}(m_L, \sigma_L^2)$.
Therefore, for equation \ref{equ:invariance_joint}, we will instead have 
$$
\begin{pmatrix}
\mathbf{f}(X)\\L[\mathbf{f}(X_L)]
\end{pmatrix}
\sim\mathcal{N}
\left(\begin{pmatrix}\mathbf{0}_{2nd}\\\mathbf{m}_{\ell}\end{pmatrix}, \begin{pmatrix}
    K & LK \\
    KL^T & LKL^T+\sigma_L^2\mathbb{I}\\
\end{pmatrix}\right),
$$
Again, the white noise will only affect the variance part of $L[\mathbf{f}(X_L)]$ and not its covariance with $\mathbf{f}(X)$.
We will then able to do the conditioning according to equation \ref{equ:normal_condtion} again and obtain
$$
  \mathbf{f}(X)|L[\mathbf{f}(X_L)]=0 \sim \mathcal{N} \left(-\mathbf{m}_\ell LK(LKL^T+\sigma^2_L\mathbb{I})^{-1}, \begin{pmatrix}
    K-LK(LKL^T+\sigma^2_L\mathbb{I})^{-1}KL^T
  \end{pmatrix}\right)
$$
Here, the parameters $\mathbf{m}_L$ and $\sigma^2_L$ will be optimised again with marginal likelihood. 
The reason we allow the noise to have a non-zero mean is that we know that the energy can only decrease so that $\frac{dE}{dt}\equiv L[\mathbf{f}(X_L)] \le 0 $, so the positive deviation of invariance from zero is not the symmetrical to its negative counterparts; we therefore introduce a learnable negative mean to accounts for that.  
The idea of introducing this soft invariance noise is that hopefully this relaxation of the contrains of invariance will allow the model to use some information of invariance to do prediction and at the same time not strictly enforcing it, which would have lead to poor performance. 
We will also use a different method to model the dissipative dynamics of the system using latent variables as shown in the section 3.4. 


\section{Learning Invariance}
Previously, we have assumed the knowledge of the dynamics equation from Newton's Law, which has allowed us to derive the energy, $E$ and the invariance equation $L$ accordingly. 
However, for the kernel to be useful in real life, we cannot assume the equations of the system beforehand, since if we have the knowledge, then we can just use a numerical ODE integrator to obtain the trajectories. 
On the other hand, even if we specify the form of invariance, we would not completely solve the system since there will still be some degree of freedom left as shown in the next chapter. 
Therefore, we should find a way for the system to learn the form of invariance from data directly.
The way we chose to do so is to parameterise our invariance function and allow the system to learn the coefficients by maximising the log marginal likelihood.

\subsection{One-dimensional system}
There are many different ways to paramerterise an unknown function, including but not limited to Fourier representation, neural networks, wavelets, splines.
As the problem is relatively bounded, we chose to use polynomial basis to paramertise the unknown function $\mathcal{L}[E]\equiv L[\mathbf{f}]$.
In other words, we will be paramerterising the $\frac{\partial E}{\partial p_i}$ and $\frac{\partial E}{\partial q_i}$ term in equation \ref{equ:LE}.
Using polynomial basis is very efficient in our cases since there will only be a few coefficents to learn.
On the other hand, if we use much more expressive splines or neural networks, we will have many more parameters to optimise, and can even easily lead to overfitting.
Not using Fourier series is because we know there can be many linear functions, and we may need a large number of Fourier series expansion to accomdate that; so the most natural choice for our task is to use polynomials.
In one-dimensional cases, for our examples, they are simple enough so that $L[\mathbf{f}]$ is separable so that we have a function of $p$ only, $f(p)$ and a different function of $q$ only, $g(q)$, such that $L[\mathbf{f}]=f(p)a+g(q)v$ so that $f, g$ can be learnt seperately.
In the one-dimensional SHM case, $f(p)=mp$ and $g(q)=kq$.
For simple pendulum, we have $f(p)=\ell p$ and $g(q)=\sin q$.
We will therefore set $$f(p)=\sum_{i=0}^{n_f}c_{f,i}p^i,\ g(q)=\sum_{i=0}^{n_g}c_{g,i}q^i,$$
where $c_{f,i}, c_{g,i}$ are the parameters we will optimise based on marginal likelihood and we will evaluate different degree of polynomial $n_f, n_g$ and see which combinations produce the best results. 

\subsection{Two-dimensional system}
In two-dimension, things are a bit more complicated. 
For example, it is very clear that in double pendulum, the invariance constraints is not seperable.
Instead, we need to consider all combinations of polynomials; i.e. multivariate polynomial.
Therefore, for two-dimension we have $L(p_1, p_2, q_1, q_2)$ being the combination of $f_1(p_1, p_2, q_1, q_2), f_2(p_1, p_2, q_1, q_2), g_1(p_1, p_2, q_1, q_2), g_2(p_1, p_2, q_1, q_2)$, $L(\mathbf{f})=f_1(p_1, p_2, q_1, q_2)a_1+f_2(p_1, p_2, q_1, q_2)a_2+g_1(p_1, p_2, q_1, q_2)v_1+g_2(p_1, p_2, q_1, q_2)v_2$.
We will paramertise each of $f_1, f_2, g_1, g_2$ with $$\sum_{i,j,k,l=0}^{n}c_{ijkl}p_1^ip_2^jq_1^kq_2^l$$
Therefore, there will be many more coefficients to be optimised.
We can calculate the number by using the combination with replacement formula
$$
C^R(n, r) = \frac{(n+r-1)!}{r!(n-1)!},
$$
where $n$ is the number of choices (4 in this case) and $r$ is the number we are choosing.
For example, if we wish to have a cubic polynomial, then we would require $\frac{(4+3-1)!}{3!(4-1)!}+\frac{(4+2-1)!}{2!(4-1)!}+\frac{(4+1-1)!}{1!(4-1)!}+\frac{(4+0-1)!}{0!(4-1)!}=35$ parameters for each polynomial so we would require $35\times 4=140$ parameters in total.

\section{Partially Observed System}
Partially observed data can be very common in dynamical system.
With examples of physical system, we can imagine not observing $p$ and only observe $q$ (of course we do not possess the knowledge that $\dot{q}=p$).
This will correpond to just observing the trajectory of a particle but has no information about the time and hence we cannot deduce velocity.
If we have a two-dimensional system, we might only observe one set of dimension due to data recording mechanism, or perhaps some situation where we project three-dimensional objects on to the wall and we can only observe the two-dimensional shadows.
In a more general sense, we can imagine if we have a system with unquantifiable friction (without prior knowledge).
We may not be able to write down the form of friction analytically due to complex interaction of the system with the enviroment (i.e. not simply a damped system as shown in equation \ref{equ:damped}, where it is analytical).
Nevertheless, the law of physics and conservation of energy will still hold in the broad sense; 
in other words, if we take into account of every single force of the interactions (including the enviroment), the total energy will be conserved so the invariance holds. 
For example, in the case of frictional forces, they arise because of the electric-magnetic forces between particles and the surface of the enviroment.
If we take into account of the particles on the surface and their interactions with the particles in our system, the energy will again be conserved. 
In practice, we can only observe the system so that the rest of the physics (interaction with the enviroment) is not observed, so it is again a partially observed problem.
To address this issue, we can use a latent variable model. 
The idea is to model the unobserved variable as a latent variable $z$ and its associated time derivative as dynamics $\dot{z}$.
For example, if we only observe $q$ in an one-dimensional system, then we can set $z=p$, a latent variable. 
Then the dynamics $f$ will include the $\dot{z}$ term, which is equivalent to $a$ if it is observed. 
In the case of two-dimensional system, where we do not observe the $p$ nor $q$ of one dimension, we will then have two latent variables $z_1, z_2$ and their associated dynamics.
We can then use the invariance condition as before to constrain the distribution of $z$ and $\dot{z}$ and therefore does inference on it using inference scheme such as Markov Chain Monte Carlo or Variational Inference.

In this report, we will focus on one particular problem: the damping case.
Here, we do not have missing inputs; instead, we have missing dynamics that accounts for the dissipative forces. 
Therefore, we invent the latent variable $z$ to represent the dissipative effect.
Now, instead of the invariance in equation \ref{equ:invariance_equation} being equal to zero, $\frac{dE}{dt}\equiv\mathcal{L}[E]=0$, we have $\frac{dE}{dt}+z=0,$ so that now we have 
\begin{equation}
\mathcal{L}[E] + z = \frac{dE}{dt} + z =\sum_{i=1}^d \frac{\partial E}{\partial p_i} \frac{\partial p_i}{\partial t}+ \sum_{i=1}^d\frac{\partial E}{\partial q_i}\frac{\partial q_i}{\partial t} + z = \sum_{i=1}^d \frac{\partial E}{\partial p_i} a_i + \sum_{i=1}^d\frac{\partial E}{\partial q_i} v_i + z,
\end{equation}
which is therefore the a linear transformation on an augmented dynamics $\mathbf{f_b}=\begin{pmatrix}
  \mathbf{f}\\z,
\end{pmatrix}$
and we will define this new linear transformation as $L_b$, which operates on $\mathbf{f_b}.$
Effectively, we treat $z$ as dynamics with equal footing with $\mathbf{a}$ and $\mathbf{v}$.
Following this argument, in a similar fashion as section 3.1, we will again put an GP prior on $z$ with RBF kernel, that is independent from $\mathbf{f}$.
Now instead of equation \ref{equ:invariance_joint}, we have 
$$
\begin{pmatrix}
\begin{pmatrix}\mathbf{f}(X)\\ z(X)\end{pmatrix}\\L_b[\mathbf{f}(X_L), z(X_L)]
\end{pmatrix}
\sim\mathcal{N}
\left(\begin{pmatrix}\mathbf{0}_{3nd}\\\mathbf{0}_{\ell}\end{pmatrix}, \begin{pmatrix}
    K & L_bK \\
    KL_b^T & L_bKL_b^T\\
\end{pmatrix}\right),
$$
where now 
$$
K = \begin{pmatrix}
  K_f & 0 \\
  0 & K_z\\
\end{pmatrix},
$$
Finally, using the normal conditional formula \ref{equ:normal_condtion}, we have 
\begin{equation}
  \begin{pmatrix}\mathbf{f}(X) \\ z(X) \end{pmatrix}|L_b[\mathbf{f}(X_L), z(X_L)]=0 \sim \mathcal{N} \left(\mathbf{0}_{3nd}, \begin{pmatrix}
    K-L_bK(L_bKL_b^T)^{-1}KL_b^T
  \end{pmatrix}\right)
\end{equation}

As we only have the data for $\mathbf{f}(X)$, we will need to marginalise the joint distribution between $\mathbf{f}$ and $z$ by integrating over all possible $z$, $\int p(\mathbf{f},z) dz$.
Fortunately, for multivariate normal distribution, if we have
$$
\begin{pmatrix}
  x \\ y
\end{pmatrix}
\sim\mathcal{N}
\left(
  \begin{pmatrix}
    m_x \\ m_y 
  \end{pmatrix},
\begin{pmatrix}
  V_{xx} & V_{xy} \\ V_{yx} & V_{yy} \\
\end{pmatrix}
\right),
$$
then 
$$
x\sim\mathcal{N}(m_x, V_{xx}).
$$
We therefore have 
$$
\mathbf{f}(X)|L_b[\mathbf{f}(X_L), z(X_L)]=0 \sim \mathcal{N}(\mathbf{0}_{2nd}, [K-L_bK(L_bKL_b^T)^{-1}KL_b^T]_{[0:2nd, 0:2nd]}),
$$
where for a matrix $M$, $M_{[0:r,0:c]}$ is a submatrix of $M$ equal to the first $r$ rows and first $c$ columns of the $M$ (zero indexed).
At the same time, we are able to get the posterior of $z$ in the same inference procedure. 
If we denote 
$$
K-L_bK(L_bKL_b^T)^{-1}KL_b^T \equiv \begin{pmatrix}
  V_{ff} & V_{fz} \\ V_{zf} & V_{zz} \\
\end{pmatrix},
$$
where $V_ff$ is the submatrix of the full covariance matrix corresponds to variance of $\mathbf{f}(X)$ (i.e. the first $2nd$ rows and $2nd$ columns as before), the rest are defined accordingly.  
Then using the normal condition formula \ref{equ:normal_condtion} once again, 
$$
z(X)|\mathbf{f}(X)=Y, L_b[\mathbf{f}(X_L), z(X_L)]=0
\sim\mathcal{N}
(V_{zf}V_{ff}^{-1}Y,\ V_zz-V_{zf}V_{ff}^{-1}V_{fz}),
$$
where $Y$ is the data for the dynamics.

\chapter{Implementation}
\section{Data Generation}
To generate the data, we use a Runge-Kutta integrator to generate data using the equation of motions for each problem. 
We also added some small Gaussian noise $10^{-8}$ to the dynamics (the time derivative of variables) when integrating to reflect the real world better as well as stablising the integrator.
Since we aim to predict the value of dynamics at any point in the input space, we use the dynamics as targets , Y. 
This is done by combining forward and backward finite difference (midpoint method) to compute the gradient. 
To illustate, $f'(x_1)=\frac{f(x_2)-f(x_0)}{2h}$, where $h$ is the spacing between $x_0, x_1$ and $x_1, x_2$ and $f'(x_1)$ is the derivative evaluate at $x_1$.
We also choose the time step to be $0.01$ to ensure sufficient accuracy for the integrator.
For training data, X, we choose a few random starting point within a range (datails depends on the task as described later) in the input space and generate data from there for a number of timesteps (which is fixed at 10 points per trajectory in this report). 
To assess the performance, we generate the test set using the same input range, we will pick three different starting points and then average the result.

\section{Evaluation Method}
We will mainly use the mean squared error (MSE) between the trajectory prediction and ground truth as our evaluation metrics. 
We also illustrate the amount of energy (using the exact expression) along the trajectory to demonstrate that invariance is obeyed.
For learning the invariance in 1D case, we can also further compare the true invariance and the learnt invariance up to a multiplicative constant.
However, it is not possible to visualise in 2D cases so we will simply assess the conservation of energy (invariance) performance.

\section{Implementation Technicalities}
We will need to add jitter, a diagonal matrix of small value, to the $LKL^T$ matrix in equation \ref{equ:invariance_condition}.
This is due to the fact that we need the invert it and it will be more stable numerically if we add a little bit of jitter in the range of $10^{-6}$ to $10^{-4}$.
As this is a common practice, its effect on the model will be in the Appendix for intereseted readers.
Similarly, for stability, when backpropagating to optimise the hyperparameters with respect to marginal likelihood, we need to narrow the range of search such that the value is more reasonable.
It is particularly important in our dynamical systems since, for example, in SHM the equation of motion is $a=-q$ and $v=p$, so that is will be a 2D plane extends to infinity;
as a result, if the GP successfully recover the true invariance, the lengthscales of certain dimensions will be very large or $\infty$ (a property of ARD kernel, essentially able to set a dimension to constant), and it would not affect the performance a lot if we limit the lengthscales for a much smaller value to avoid instability.
Therefore, here I would set the range of both lengthscales and variable to be around the two to three times the maximum range possible.
For example, if we have maximum range of $\pm 3$, my upper bound for search is set to $10$ and lower bound is $10^{-3}$.
Another change we made in two-dimensional SHM is to scale the data using MinMax scaler to squash them to be between $\pm 1$. 
This is done since the polynomial behaves better around this region.

\subsection*{Invariance points $X_L$}
In low dimensions, we could simply allow $X_L$ to be a evenly spaced grid in the input space, and we will call the number of points in each dimension the \textbf{invariance density}.
So that if I have an invariance density of $40$, it means my $X_L$ will contain $1600$ points. 
However, conditioning on a grid of invariance points is simply not scalable in high dimensions. 
If we have an invariance density of 10 in a $\pm 5$ range for 2D SHM (spacing of 1), then we will have 10000 points. 
We can easily run out of memory and also taking the matrix inverse are very difficult and time consuming.
As a result, to make the approach scalable to high dimensional space, we will use the idea of local invariance. 
The idea is straightforward, we simply sample uniformly (20-80 points) in the local region around the training and testing points and set them as $X_L$ (we choose a torus shape around the data points so that we will not have invariance points too close to the data points, making it unstable).
Note that we only sampled once, so that the same local grid will be centred on every training and testing points; this is done to remove the randomness for stability and dependence on the data.
We will still have the global grid invariance points as before (but fewer) so that we can still impose some degree of invariance if the test points are very far from the training points.
This works on the assumption that only the region around testing points will be the most important to predict in terms of conditioning; and to use the same kernel, we also have to condition on the region around training points, which is also important to help maximising the marginal likelihood objective when we are learning the invariance function.
Another evidence that supports the use of local invariance, which have much fewer points than using a grid, is demonstrated below where we show that there is a diminishing return in increasing the number of invariance points so that actually not that many invariance points is needed to achieve a good performance. 

\section{One-dimensional System}
\subsection{Linear SHM}
We will first examine one of the most simple dynamical system, an one-dimensional simple harmonic motion (SHM). 
An example would be a mass spring system as shown in figure with mass $m$ and spring constant $k$, and an example trajectory is shown in figure \ref{fig:shm_trajectory}. 
The $q$ is the displacement of the block from its origin, and $p$ would be the instanteous velocity.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/shm.pdf}
        \caption{SHM illustration}
        \label{fig:shm_diagram}
\end{figure}

The equation of motion of the system is $$m\frac{d^2q}{dt^2}=-kq.$$
And the analytical solution would be of the form $q=A\sin(\omega_0t+\phi),$ where $\omega=\frac{k}{m}$ and $A, \phi$ depends on the initial condition, which dictates the amplitude and phase of the motion. 
For this case, we have dynamics 
$$
\mathbf{f}(\mathbf{x})=\begin{pmatrix}
    a(\mathbf{x})\\
    v(\mathbf{x})\\
\end{pmatrix}
$$

\begin{figure}[H] 
  \includegraphics[width=0.7\linewidth]{../codes/figures/shm_trajectory_1D.pdf}
  \centering
  \caption{Example trajectory of SHM}
  \label{fig:shm_trajectory}
\end{figure}

We also have the energy, $E=\frac{mp^2}{2}+\frac{kq^2}{2},$
Therefore, to obtain our invariance $L$, we use the conservation of energy $\frac{dE}{dt}=0$ so we finally have $$L[\mathbf{f}]=mpa+kqv=0$$
While $L$ is not able to be put into a matrix form using our representation, it is a linear operator as shown below. 
If we have $$X_L\equiv\begin{pmatrix}
    \mathbf{x}_{L,1}\\
    \vdots\\
    \mathbf{x}_{L,\ell}\\
\end{pmatrix}=\begin{pmatrix}
    q_{L,1} & p_{L,1}\\
    \vdots & \vdots \\
    q_{L,\ell} & p_{L,\ell}\\
\end{pmatrix}\equiv\begin{pmatrix}
    \vdots & \vdots\\
    q_L & p_L\\
    \vdots & \vdots\\
\end{pmatrix},$$
then we have 
$$
L([\mathbf{f}(X_L)]) = \begin{pmatrix}
   mp_{L,1}a(q_{L,1},p_{L,1}) + kq_{L,1}v(q_{L,1},p_{L,1})\\ 
   \vdots \\
   mp_{L,\ell}a(q_{L,\ell},p_{L,\ell}) + kq_{L,\ell}v(q_{L,\ell},p_{L,\ell})\\ 
\end{pmatrix},
$$
which can be readily checked to be linear.
Combine with original GP prior assumption, we have 
$$
\begin{pmatrix}
    \mathbf{f}(X)\\
    L([\mathbf{f}(X_L)])\\
\end{pmatrix}
\sim\mathcal{N}
\left(\begin{pmatrix}
    0_{2n}\\0_{\ell}
\end{pmatrix},\begin{pmatrix}
   A & B \\
   C & D\\ 
\end{pmatrix}\right),
$$
where
$$
\begin{gathered}
A=K(X,X), B=\begin{pmatrix}
    K_{RBF,a}(X, X_L) \\ K_{RBF,v}(X, X_L) \\
\end{pmatrix}\odot \begin{pmatrix}
    mP_L \\ kQ_L
\end{pmatrix}, C=B^T,\\ D=K_{RBF,a}(X_L, X_L)\odot m^2(p_L\otimes p_L) + K_{RBF,v}(X_L, X_L)\odot k^2(q_L\otimes q_L),
\end{gathered}
$$
where $\odot$ is the elementwise product and $\otimes$ is the Kronecker product so that 
$$
P_L=\begin{pmatrix}
  p_{L,1}  & \dots & p_{L,\ell}  \\
  \vdots & \text{repeats n rows} &  \vdots\\
  p_{L,1}  & \dots & p_{L,\ell}  \\
\end{pmatrix},
Q_L=\begin{pmatrix}
  q_{L,1}  & \dots & q_{L,\ell}  \\
  \vdots & \text{reqeats n rows} &  \vdots\\
  q_{L,1}  & \dots & q_{L,\ell}  \\
\end{pmatrix}
$$
and we have 
$$
p_L\otimes p_L=\begin{pmatrix}
  p_{L,1}^2 & p_{L,1}p_{L,2} & \dots & p_{L,1}p_{L,\ell} \\
  \vdots & \vdots & \vdots & \vdots \\
  p_{L,\ell}p_{L,1} & p_{L,\ell}p_{L,2} & \dots & p_{L,\ell}^2 \\
\end{pmatrix},
q_L\otimes q_L=\begin{pmatrix}
  q_{L,1}^2 & q_{L,1}q_{L,2} & \dots & q_{L,1}q_{L,\ell} \\
  \vdots & \vdots & \vdots & \vdots \\
  q_{L,\ell}q_{L,1} & q_{L,\ell}q_{L,2} & \dots & q_{L,\ell}^2 \\
\end{pmatrix},
$$ 
These matrix expressions are derived as follows by computing the covariance manually. 
For $B$, we wish to calculate 

\begin{align*}
B_{ij} &= \mathrm{Cov}(\mathbf{f}(X), L[\mathbf{f}(X_L)])_{ij} \\
       &= \mathrm{Cov}(\mathbf{f}(X)_i, L[\mathbf{f}(X_L)]_j) \\ 
       &= \begin{cases}
        \mathrm{Cov}(a(q_i, p_i), mp_{L,j}a(q_{L,j},p_{L,j}) + kq_{L,j}v(q_{L,j},p_{L,j})) & i\le n \\ 
        \mathrm{Cov}(v(q_i, p_i), mp_{L,j}a(q_{L,j},p_{L,j}) + kq_{L,j}v(q_{L,j},p_{L,j})) & i>n \\ 
       \end{cases} \\
       &= \begin{cases}
        K_{RBF,a}(\mathbf{x}_i, \mathbf{x}_{L,j}) mp_{L,j} & i\le n \\ 
        K_{RBF,v}(\mathbf{x}_i, \mathbf{x}_{L,j}) kq_{L,j} & i>n \\ 
       \end{cases}, \\
\end{align*}
and hence we have the form above. 
Similarly for $D$, we have
\begin{align*}
D_{ij} &= \mathrm{Cov}(L[\mathbf{f}(X_L)], L[\mathbf{f}(X_L)])_{ij} \\
       &= \mathrm{Cov}(mp_{L,i}a(q_{L,i},p_{L,i}) + kq_{L,i}v(q_{L,i},p_{L,i}), mp_{L,i}a(q_{L,i},p_{L,i}) + kq_{L,i}v(q_{L,i},p_{L,i})) \\
       &= m^2p_{L,i}p_{L,j}K_{RBF,a}(\mathbf{x}_{L,i},\mathbf{x}_{L,j}) + k^2q_{L,i}q_{L,j}K_{RBF,v}(\mathbf{x}_{L,i},\mathbf{x}_{L,j})
\end{align*}
using the bilinear property of the covariance operator and the fact that $v$ and $a$ are independent.
Since we assume invariance holds on these invariance points, we will condition on $L([\mathbf{f}(X_L)])=0.$
Now we can simply use the Gaussian conditional formula to obtain the Schur Complement using equation \ref{equ:normal_condtion}.
$$
\mathbf{f}(X)|L[\mathbf{f}(X_L)]=0\sim\mathcal{N}(0_{2n},A-BD^{-1}C),
$$
we will then call the resulting covariance our Invariance Kernel for 1D SHM, $K_L$.

Now we look at how it works in practice.
Without loss of generality, we choose $m=k=1$ and $g=\ell=1$ for 1D and 2D SHM and simple pendulum for simplicity of presentation. 
For the data we only have one trajectory starting randomly in $-3\le q\le3$ and $p$ to be between $\pm 0.3$.
We chose $p$ to be smaller so that the future trajectory does not overshoot the upper bound of $q$ too much. 
We allow the integrator to run for $0.1$ seconds so there will be in total 10 training points (10 time steps). 
We will then draw three test points from the same range as training, to assess the performance of the predictive power for 3 seconds (300 points) and average the MSE over the three trajectories. 
In figure \ref{fig:shm_prediction}, \ref{fig:shm_energy}, we have chosen one trajectory at random to illustate the predictions as well as the energy along the trajectory.
For this task, we have used a jitter amount of $10^{-5}$ and an invariance density of 20 between $\pm 3$ in both $p$ and $q$ direction.
We have also plotted the posterior distribution of using invariance kernel and compare that to using naive RBF kernel in figure \ref{fig:posterior_shm}.
We can see the invariance has much better generalisability.
For learning the invariance, we test all combinations of polynomials of $f(p)$ and $g(q)$, each with degree 0, 1, 2, 3 (i.e. up to cubic term) so there will be 16 of them to optimise, and we choose the pair with the highest marginal likelihood. 
We initialise the coefficients with normal distribution with variance of $10^{-3}$, we also restrict the search space to be between $\pm 1$.
We have also added a Laplace prior on the coefficients with variance 0.1 since we expect most coefficients would be zero and Laplace prior encourage sparse solution.
The learnt $f(p)$ and $g(q)$ is plotted in figure \ref{fig:shm_learnt}.
The performance is summarised in table \ref{tab:shm_performance}.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l }
\hline
Method           & RBF & Known Invariance&  Learnt Invariance\\
  \hline
Log Marginal Likelihood & 67.67 & 82.00 & 79.24 \\
MSE & 0.0950 & 0.0017 & 0.0027 \\
                    \hline
  \end{tabular}
  \caption{SHM performance}
  \label{tab:shm_performance}
\end{table}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{../codes/figures/shm_predicted.pdf}
        \caption{SHM predicted trajectory}
        \label{fig:shm_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/shm_energy.pdf}
  \centering
  \caption{Energy conservation for SHM}
  \label{fig:shm_energy}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/shm_learnt_over_time.pdf}
  \centering
  \caption{Learnt invariance for SHM}
  \label{fig:shm_learnt}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../codes/figures/posterior_shm_rbf.pdf}
        \caption{SHM RBF posterior}
        \label{fig:posterior_shm_rbf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{../codes/figures/posterior_shm_invariance.pdf}
         \caption{SHM invariance posterior}
         \label{fig:posterior_shm_invariance}
     \end{subfigure}
        \caption{SHM posteriors}
        \label{fig:posterior_shm}
\end{figure}



We can clearly see the naive RBF kernel failed to learn at all as it does not have enough example data to genealise, as we can see from \ref{fig:posterior_shm_rbf}, so that it is only able to predict well in the region where the data are. 
On the other hand, from \ref{fig:posterior_shm_invariance} and the prediction performance, we can see the invariance kernel has much better generalisation ability.
From the energy plot in figure \ref{fig:shm_energy}, we again see both the known and learnt invariance obeys the invariance very well while the RBF kernel does not at all.
In this case, the learnt invariance performs almost exactly as the known form, as we can also see in figure \ref{fig:shm_learnt}, and has successfully recovered the physics as it trains. 


\subsection{Nonlinear Pendulum}
The idea is pretty much the same for nonlinear system compared to linear; however, the fitting of statistical model is expected to be more difficult.
A simple nonlinear system in every day life is a simple pendulum as shown in figure below, where $q$ is the angle of displacement from the vertical dotted line, and $p$ is the angular velocity.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.3\linewidth]{../figures/pendulum.pdf}
        \caption{Pendulum illustration}
        \label{fig:pendulum_diagram}
\end{figure}

The governing equation is 
$$
\frac{d^2q}{dt^2}=-\frac{g}{\ell}\sin q, 
$$
and we have example trajectory in \ref{fig:pendulum_trajectory}, where we can see the effect of nonlinearity on the shape. 
The nonlinear dynamics arises from the sine term above, which will complicate the derivation for invariance kernel slightly. 
However, since $\sin x \approx x$ at small angle, this system is approximately linear under small displacement. 
There is no analytical solution to this nonlinear problem in general, besides the small displacement case, which will be one-dimensional SHM.

\begin{figure}[H] 
  \includegraphics[width=0.7\linewidth]{../codes/figures/pendulum_trajectory_1D.pdf}
  \centering
  \caption{Example trajectory of pendulum}
  \label{fig:pendulum_trajectory}
\end{figure}

This time we have energy, $E=\frac{m\ell^2p^2}{2}+mg\ell(1-\cos q)$, and by setting its time derivative to 0, we have 
$$L(a, v)=\frac{dE}{dt}=m\ell^2pa+mg\ell(\sin q)v=0.$$
If we cancel out the common term $m\ell$ since their product cannot be zero, we have $$L(a,v)=\ell pa+g(\sin q)v=0$$
Most of the terms are unchanged from the linear case. 
However, this time, 
$$
\begin{gathered}
B=\begin{pmatrix}
    K_{RBF,a}(X, X_L) \\ K_{RBF,v}(X, X_L) \\
\end{pmatrix}\odot \begin{pmatrix}
    \ell P_L \\ g\sin(Q_L)
\end{pmatrix},\\ D=K_{RBF,a}(X_L, X_L)\odot \ell^2(p_L\otimes p_L) + K_{RBF,v}(X_L, X_L)\odot g^2(\sin(q_L)\otimes \sin(q_L)),
\end{gathered}
$$
where 
$$
\sin(Q_L) = g\begin{pmatrix}
  \sin(q_{L,1})  & \dots & \sin(q_{L,\ell})  \\
  \vdots & \text{reqeats n rows} &  \vdots\\
  \sin(q_{L,1})  & \dots & \sin(q_{L,\ell})  \\
\end{pmatrix},
$$
$$
\sin(q_L)\otimes \sin(q_L)=\begin{pmatrix}
  \sin(q_{L,1})^2 & \sin(q_{L,1})\sin(q_{L,2}) & \dots & \sin(q_{L,1})\sin(q_{L,\ell}) \\
  \vdots & \vdots & \vdots & \vdots \\
  \sin(q_{L,\ell})\sin(q_{L,1}) & \sin(q_{L,\ell})\sin(q_{L,2}) & \dots & \sin(q_{L,\ell})^2 \\
\end{pmatrix},
$$
derived in exactly the same way as the linear case. 
We can look at the different priors in figure \ref{fig:priors}, which showcases the baseline kernel RBF, SHM invariance kernel, as well as the pendulum invariance kernel.

We can see while the mean remains zero, the uncertainty parts reflects the form of invariance; such as the plane of SHM and the sinusoidal nature of pendulum invariance in \ref{fig:prior_pendulum_invariance} (left).

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../codes/figures/prior_shm_rbf.pdf}
        \caption{RBF prior}
        \label{fig:prior_shm_rbf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.8\linewidth}
         \centering
         \includegraphics[width=\linewidth]{../codes/figures/prior_shm_invariance.pdf}
         \caption{SHM invariance prior}
         \label{fig:prior_shm_invariance}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.8\linewidth}
         \centering
         \includegraphics[width=\linewidth]{../codes/figures/prior_pendulum_invariance.pdf}
         \caption{Pendulum invariance prior}
         \label{fig:prior_pendulum_invariance}
     \end{subfigure}
        \caption{Priors}
        \label{fig:priors}
\end{figure}

For the implementation of pendulum invariance model, the experiment setup is essentially exactly the same as the SHM case. 
However, since this time the dynamics is more complicated, we will draw three different starting points for training between $q=\pm 150^{\circ}$ and $\pm 10^{\circ}$ for $p$, so in total 30 points.
This time, since it is a harder task, we will use an invariance density of 30 between $\pm 150^{\circ}$.
We again have the prediction performance summarised in table \ref{tab:pendulum_performance} as well as figures \ref{fig:pendulum_prediction}, \ref{fig:pendulum_energy} and the posteriors in \ref{fig:posterior_pendulum}.
The learnt polynomial is in \ref{fig:pendulum_learnt}.

\begin{table}[H]
  \centering
  \begin{tabular}{ l l l l}
    \hline
Method           & RBF & Known Invariance&  Learnt Invariance\\
                    \hline
Log Marginal Likelihood & 299.12 & 331.66 & 325.76 \\
MSE & 0.0021 & 0.0009 & 0.0006 \\
    \hline
  \end{tabular}
  \caption{Pendulum performance}
  \label{tab:pendulum_performance}
\end{table}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{../codes/figures/pendulum_predicted.pdf}
        \caption{pendulum predicted trajectory}
        \label{fig:pendulum_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/pendulum_energy.pdf}
  \centering
  \caption{Energy conservation for pendulum}
  \label{fig:pendulum_energy}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/pendulum_learnt_over_time.pdf}
  \centering
  \caption{Learnt invariance for pendulum}
  \label{fig:pendulum_learnt}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../codes/figures/posterior_pendulum_rbf.pdf}
        \caption{pendulum RBF posterior}
        \label{fig:posterior_pendulum_rbf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{../codes/figures/posterior_pendulum_invariance.pdf}
         \caption{pendulum invariance posterior}
         \label{fig:posterior_pendulum_invariance}
     \end{subfigure}
        \caption{pendulum posteriors}
        \label{fig:posterior_pendulum}
\end{figure}

From table \ref{tab:pendulum_performance}, figure \ref{fig:pendulum_prediction}, \ref{fig:pendulum_energy}.
This time, again the RBF kernel performs poorly in terms of perdictive performance.
The known kernel predicts very well again, almost exactly the same as the ground truth.
However, this time, the learnt invariance is slightly worse in predicting; this is expected since we are approximating a sine function with a cubic polynomial, and so some level of deviation is expected.
As we can see from figure \ref{fig:pendulum_learnt}, the learning is increasingly accurate as it trains over time.
From the posteriors in \ref{fig:posterior_pendulum}, we see RBF generalise much better due to extra data points, but it is still very far from giving a reasonable prediction. 

We can also compare the data efficiency in this case as shown in figure \ref{fig:data_efficiency}. 
We see that with invariance kernel, it is at least ten times more data efficient than the baseline RBF.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/data_efficiency.pdf}
  \centering
  \caption{Data efficiency for pendulum}
  \label{fig:data_efficiency}
\end{figure}

\section{Invariance Density}
Here we will to explore how different invariance density affect the performance of the kernel as well as the degree of freedom.
Since one-dimensional SHM is trival to learn, we test it on the nonlinear pendulum case, which will illustrate the point better. 
We will use different invariance density using the same training data and three different trajectories of testing data.
We again calculate the MSE of trajectory prediction as well as the percentage of energy deviation from the starting point of three different trajectories and average over.
The results is shown in figure \ref{fig:vary_invariance_density}.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/vary_invariance.pdf}
  \centering
  \caption{Effect of varying invariance density}
  \label{fig:vary_invariance_density}
\end{figure}

We can see that for the marginal likelihood, it increases as the number of invariance points increases, which is expected since we have more information/evidence to support the data.
However, MSE first decreases then increases.
The first decrease makes sense since with more invariance points, we should in principle predict better.
However, the increase is counter-intuitive.
We conclude that there is actually not that big a difference and it is possibly due to the numerical instability as we increase the size of the matrix with increasing invariance density.  
We also see the deviation of energy decreases as we increase the number of invariance points, which makes sense because we are imposing stronger invariance contraints in the input space by increasing the invariance density.
Lastly, we see the degree of freedom does not vary much either, while in theory it should decrease with increasing invariance density due to more constraints on the function.

Overall, we see there is not that big an advantage to increase our invariance density beyond a certain point due to diminishing return in performance and $N^6$ computational cost.
Therefore, from this point we will choose the smallest invariance density that give reasonable performance.  


\section{Damped System}
For the data, we will model the system as a damped SHM or pendulum with linear velocity dependent force.
If we denote $\omega_0^2=k/m$ or $\omega_0^2=g/\ell$
We will have the equation of motion for SHM and pendulum respectively
$$
\frac{d^2q}{dt^2}+2\gamma\frac{dq}{dt}+\omega_0^2q=0; \frac{d^2q}{dt^2}+2\gamma\frac{dq}{dt}+\omega_0^2\sin q=0 ,
$$
and the $\gamma$ is the damping factor that controls how much frictional force there is. 
Note that we will only focus on underdamping case ($\gamma < \omega_0^2$)so that the system still oscillate but with gradually reduced amplitude instead of critical damped ($\gamma=\omega_0^2$) or overdamped ($\gamma>\omega_0^2$) case where the system simply slowly decays to still.
An example trajectory for damped SHM and damped pendulum is shown below in figure \ref{fig:damped_trajectory}.
For the damping, we will choose $\gamma= 0.1$, which is fairly strongly damped as shown in figure \ref{fig:damped_pendulum_trajectory}, \ref{fig:damped_shm_trajectory}. 

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.5\linewidth]{../codes/figures/damped_shm_trajectory_1D.pdf}
        \caption{Example trajectory of damped SHM}
        \label{fig:damped_shm_trajectory}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.5\linewidth]{../codes/figures/damped_pendulum_trajectory_1D.pdf}
        \caption{Example trajectory of damped pendulum}
        \label{fig:damped_pendulum_trajectory}
     \end{subfigure}
        \caption{Example trajectories of damped systmes}
        \label{fig:damped_trajectory}
\end{figure}

\subsection{Damped SHM}
For damped SHM, we have exactly the same setup as the original undamped SHM.
The results is again summarised in table \ref{tab:damped_shm_performance} and figure \ref{fig:damped_shm_prediction}, \ref{fig:damped_shm_energy}.
The learnt $f(p)$ and $g(q)$ is shown in figure \ref{fig:damped_shm_learnt}.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l l l}
    \hline
Method           & RBF & Known Invariance&  Learnt Invariance & Known Invariance (latent) & Learnt Invariance (latent)\\
  \hline
Log Marginal Likelihood & 636 & 647 & 646 & 649 & 653 \\
MSE & 0.00142 & 0.00130 & 0.00134 & 0.00091 & 0.00101 \\
    \hline
  \end{tabular}
  \caption{Damped SHM performance}
  \label{tab:damped_shm_performance}
\end{table}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{../codes/figures/damped_shm_predicted.pdf}
        \caption{Damped SHM predicted trajectory}
        \label{fig:damped_shm_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/damped_shm_energy.pdf}
  \centering
  \caption{Energy conservation for damped SHM}
  \label{fig:damped_shm_energy}
\end{figure}


\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/damped_shm_learnt_over_time.pdf}
  \centering
  \caption{Learnt invariance for damped SHM}
  \label{fig:damped_shm_learnt}
\end{figure}

We can see that the RBF performance is still poor as before.
However, this time the invariance kernel does not perform as well as before. 
This is expected since we still enforce some degree of invariance, even though it is noisy. 
However, it is clear that it is making use of the knowledge of the invariance since the performance is fairly good.
For energy consevation, it is reassuring that the invariance kernel is still remains invariant, while the learnt invariance might deviate a little since the extra parameters in the polynomial might fit to the damped data; but overall it conserves energy relatively well compare to the ground truth.
Also, when we are learning the invariance, it again successfully recover the physics even under the effect of damping.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/latent_damped_shm.pdf}
  \centering
  \caption{Latent variable distribution}
  \label{fig:latent_damped_shm}
\end{figure}



\subsection{Damped Pendulum}
Again, the setting of damped pendulum is the same as the undamped pendulum.
The results is summarised in table \ref{tab:damped_pendulum_performance} and figure \ref{fig:damped_pendulum_prediction}, \ref{fig:damped_pendulum_energy}.
With the learnt polynomial in \ref{fig:damped_pendulum_learnt}.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l l l}
    \hline
Method           & RBF & Known Invariance&  Learnt Invariance & Known Invariance (latent) & Learnt Invariance (latent)\\
  \hline
Log Marginal Likelihood & 516 & 525 & 525 & 548 & 522 \\
MSE & 0.0012 & 0.0008 & 0.0008 & 0.0008 & 0.0008 \\

    \hline
  \end{tabular}
  \caption{Damped Pendulum Performance}
  \label{tab:damped_pendulum_performance}
\end{table}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{../codes/figures/damped_pendulum_predicted.pdf}
        \caption{damped pendulum predicted trajectory}
        \label{fig:damped_pendulum_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/damped_pendulum_energy.pdf}
  \centering
  \caption{Energy conservation for damped pendulum}
  \label{fig:damped_pendulum_energy}
\end{figure}


\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/damped_pendulum_learnt_over_time.pdf}
  \centering
  \caption{Learnt invariance for damped pendulum}
  \label{fig:damped_pendulum_learnt}
\end{figure}

We see similar performance as in damped SHM, with invariance kernels performs slightly worse than undamped version but still much better than the RBF.
The energy conservation still holds mostly for invariance kernels.
However, this time, the learnt polynomial is not as good as the undamped case with much more deviation; this is expected due to the damping.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/latent_damped_pendulum.pdf}
  \centering
  \caption{Latent variable distribution}
  \label{fig:latent_damped_pendulum}
\end{figure}

\section{Two-dimensional System}
A two-dimensional system is not that different from an one-dimensional system. The major difference being there will be two more variables. 
We will again look at two simple examples, a linear two-dimensional SHM, and a nonlinear double pendulum.

\subsection{Linear SHM}
A simple extension to one-dimensional SHM to two-dimensional is just allowing the spring to be at an angle so that the system is now in a two-dimensional space as shown in figure \ref{fig:shm_2d_diagram}, and the system is still constrained in the plane.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.4\linewidth]{../figures/2dshm.pdf}
        \caption{2D SHM illustration}
        \label{fig:shm_2d_diagram}
\end{figure}

Again, the equations are simple that we have two equations for the two coordinates.
$$
\begin{cases}
    \frac{d^2{q_1}}{dt^2} = -\frac{k}{m}q_1\\
    \frac{d^2{q_2}}{dt^2} = -\frac{k}{m}q_2\\
\end{cases}
$$
The analytical solution is exactly the same, but now there are two of them with differing amplitudes and phases depending on the initial condition. 
And now we have $$\mathbf{f}(X)=\begin{pmatrix}
    \mathbf{a_1}(X)\\
    \mathbf{a_2}(X)\\
    \mathbf{v_1}(X)\\
    \mathbf{v_2}(X)\\
\end{pmatrix},$$
where $\mathbf{a_1}$ and $\mathbf{a_2}$ are the dynamics or time derivative for $\mathbf{p_1}$ and $\mathbf{p_2}$ respectively; similarly $\mathbf{v_1}$ and $\mathbf{v_2}$ are the time derivative of $\mathbf{q_1}$ and $\mathbf{q_2}$.
In this system, the energy is the sum of energy in the two directions so $E=\frac{m(p_1^2+p_2^2)}{2}+\frac{k(q_1^2+q_2^2)}{2}$ and so the invariance $L(a_1, a_2, v_1, v_2)=\frac{dE}{dt}=mp_1a_1+mp_2a_2+kq_1v_1+kq_2v_2=0.$
Now our naive baseline GP has the kernel
$$
K(X,X')=\begin{pmatrix}
K_{RBF,a_1}(X,X') & 0 & 0 & 0 \\
0 & K_{RBF,a_2}(X,X') & 0 & 0 \\
0 & 0 & K_{RBF,v_1}(X,X') & 0 \\
0 & 0 & 0 & K_{RBF,v_2}(X,X') \\
\end{pmatrix}.
$$
We will then have the joint distribution of
$$
\begin{pmatrix}
    \mathbf{f}(X)\\L[\mathbf{f}(X_L)]\\
\end{pmatrix}
\sim\mathcal{N}
\left(
\begin{pmatrix}
    0_{4n} \\ 0_{\ell}
\end{pmatrix},
\begin{pmatrix}
    A & B \\
    C & D\\
\end{pmatrix}
\right),
$$
with 

$$
A=K(X, X'), 
B=\begin{pmatrix}
    K_{RBF, a_1}\\
    K_{RBF, a_2}\\
    K_{RBF, v_1}\\
    K_{RBF, v_2}\\
\end{pmatrix}\odot
\begin{pmatrix}
mP_{1L}  \\
mP_{2L}  \\
kQ_{1L}  \\
kQ_{2L}  \\
\end{pmatrix},
C=B^T 
$$

\begin{gather*}
D=K_{RBF,a_1}m^2\odot(p_{1L}\otimes p_{1L}) + K_{RBF,a_2}m^2\odot(p_{2L}\otimes p_{2L})\\+K_{RBF,v_1}k^2\odot(q_{1L}\otimes q_{1L})+K_{RBF,v_2}k^2\odot(q_{2L}\otimes p_{2L})
\end{gather*}

where the terms are defined the exact same way as before, but now with respect to different coordinates, and the derivation are also the same.
We will then again take the Schur Complement as in equation \ref{equ:normal_condtion} as before.

For implementation, we again use the same set of input space as the one-dimensional space, but double the dimensions. 
Here we actaully have the input space of $q_1, q_2$ being between $\pm 5$ and $p_1, p_2$ between $\pm 0.5$. 
This is done so that when we scale the data with MinMax scaler, the points will be less clustered.
We use 30 points in this setting.
\textbf{One very important note} is that when we are learning invariance, we initialise the hyperparameters of the base RBF kernel (lengthscales and variance local grids as well as likelihood variance) at the values trained by the known invariance.
We also intialise the polynomial coefficients at the true theoretical values.
These are done due to the fact the optimiser is often stuck at local minima.
We intialise the values there so that if it is indeed the true invariance, the optimiser should regonise it and not optimise too much since that is roughly the true minimum.
Therefore, we would expect to get similar performance and marginal likelihood.
We use three different ways to verify indeed it is learning the correct invariance.
\begin{enumerate}
  \item Fix the polynomials coefficients, which are randomly initialised, of 50 points and optimise the other hyperparameters (lengthscale, variance etc.); then we will evaluate their performance and marginal likelihood compare to the correct one; which is expected to be the best in both measure. 
  \item We will then find the correlation between the marginal likelihood and predictive performance, which is expect to be positive.
  \item Lastly, we will allow the polynomial coefficients to be optimised, and we should see better marginal likelihood with better perdicitve performance or similar as fixing the polynomial coefficients.
\end{enumerate}

We can also assess its energy conserving performance since the true invariance will allows the energy to be conserved. 

The results are summarised in table \ref{tab:SHM_2d_performance} and figure \ref{fig:shm_2d_prediction}, \ref{fig:shm_2d_energy}.
Since it is difficult to visualise 4 dimensional space, we will plot them individually.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l}
    \hline
Method           & RBF & Known Invariance&  Learnt Invariance\\
  \hline
Log Marginal Likelihood & 430.62 & 478.70 & 475.42  \\
MSE & 0.0271 & 0.0035 & 0.0035 \\
                    \hline
  \end{tabular}
  \caption{Two-dimensional SHM Invariance performance}
  \label{tab:SHM_2d_performance}
\end{table}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/shm_2d_predicted.pdf}
  \centering
  \caption{Two-dimensional SHM prediction}
  \label{fig:shm_2d_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/shm_2d_energy.pdf}
  \centering
  \caption{Two-dimensional SHM energy}
  \label{fig:shm_2d_energy}
\end{figure}

Firstly, we can see that the invariance kernel still performs very well compared to RBF, although not as good as the 1D case as expected since higher dimensional problem is deemed to be more difficult.
We can see the learnt invariance again is almost exactly the same as the known form, which is reassuring.
Also, the conservation of energy is again pretty well obeyed.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/shm_2d_polynomial.pdf}
  \centering
  \caption{Two-dimensional SHM learnt invariance}
  \label{fig:shm_2d_poly}
\end{figure}

Lastly, we confirm the above check to make sure we are indeed learning the correct invariance.
We can see the freely optimised polynoimal is exactly the same as the fixed theoretically correct value, and are much better than the randomly initialised invariance. 
We also found a Pearson correlation between log marginal likelihood and MSE of $-0.4274$ with a p value of $0.0020$, therefore, it is clear we are learning the correct invariance. 

\subsection{Nonlinear Double Pendulum}
Now we will introduce double pendulum, which is a fairly nonlinear system and quite complicated. 
It has two mass blobs $m_1$ and $m_2$ as well as two lengths for the pendlum stem $\ell_1$ and $\ell_2$ as shown in figure \ref{fig:double_pendulum_diagram}.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.4\linewidth]{../figures/doublependulum.pdf}
        \caption{Double Pendulum illustration}
        \label{fig:double_pendulum_diagram}
\end{figure}

The defining equations are as follows:
$$
\begin{cases}
\frac{d^2q_1}{dt^2}=\frac{-g\left(2 m_{1}+m_{2}\right) \sin q_1-m_{2} g \sin \left(q_1-2 q_2\right)-2 \sin \left(q_1-q_2\right) m_{2}\left(p_2^{2} l_{2}+p_1^{2} l_{1} \cos \left(q_1-q_2\right)\right)}{l_{1}\left(2 m_{1}+m_{2}-m_{2} \cos \left(2 q_1-2 q_2\right)\right)} \\
\frac{d^2q_2}{dt^2}=\frac{2 \sin \left(q_1-q_2\right)\left(p_1^{2} l_{1}\left(m_{1}+m_{2}\right)+g\left(m_{1}+m_{2}\right) \cos q_1+p_2{ }^{2} l_{2} m_{2} \cos \left(q_1-q_2\right)\right)}{l_{2}\left(2 m_{1}+m_{2}-m_{2} \cos \left(2 q_1-2 q_2\right)\right)}
\end{cases}
$$
As we can see, it is very complicated in term of the form.
We also have form of energy 
$$
E = -(m_1+m_2)gl_1\cos q_1-m_2gl_2\cos q_2+ \frac{m_1l^2_1p_1^2}{2}+\frac{m_2}{2}(l^2_1p_1^2+l^2_2p_2^2+2l_1l_2p_1p_2\cos(q_1-q_2))
$$
While the form is more complicated, the underlying principle to construct the invariance kernel. 
We again differentiate with respect to time and set it to zero to obtain the invariance equation to obtain 
\begin{gather*}
L(a_1, a_2, v_1, v_2)=\frac{dE}{dt} = (m_1+m_2)gl_1\sin\theta_1v_1+m_2gl_2\sin\theta_2v_2+m_1l_1^2\dot{\theta}_1a_1+\\m_2(l_1^2\dot{\theta}_1a_1+l_2^2\dot{\theta}_2a_2+l_1l_2(\dot{\theta}_2\cos(\theta_1-\theta_2)a_1+\dot{\theta}_1\cos(\theta_1-\theta_2)a_2-\dot{\theta_1}\dot{\theta_2}\sin(\theta_1-\theta_2)(v_1-v_2)))=0
\end{gather*}
The form of the invariance matrix is too cubersome to write down, but the derivation is as straightforward as before. 

For our experiment, with double pendulum, we have angles being between $\pm 60^\circ$ and angular velocity between $\pm 10^\circ$.
We also set $m_1=m_2=\ell_1=\ell_2=g=1.$
We again initialised the polynomial coefficients at the correct theoretical value.
However, this time, because we are approximating, we do not expect that intial value to perform great so we can only used the third evaluation procedure to verify it is learning the invariance along with its energy conserving performance.
The results are summarised in table \ref{tab:double_pendulum_performance} and figure \ref{fig:double_pendulum_prediction}, \ref{fig:double_pendulum_energy}.
Again we used 30 points to train, but this time we do not use scaling since perhaps due to its nonlinear nature, it made the performance much worse by possibly modifying its underlying distribution.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l}
    \hline
Method           & RBF & Known Invariance&  Learnt Invariance\\
  \hline
Log Marginal Likelihood & 783.46 & 838.41 & 869.09  \\
MSE & 0.0040 & 0.0004 & 0.0018 \\
    \hline
  \end{tabular}
  \caption{Double pendulum invariance performance}
  \label{tab:double_pendulum_performance}
\end{table}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/double_pendulum_predicted.pdf}
  \centering
  \caption{Double pendulum prediction}
  \label{fig:double_pendulum_prediction}
\end{figure}

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/double_pendulum_energy.pdf}
  \centering
  \caption{Double pendulum energy}
  \label{fig:double_pendulum_energy}
\end{figure}
Now it is interesting to obsesrve that the marginal likelihood of the learnt form is even better than known form.
This is likely due to "overfitting" of the large number of parameters from the polynomials (140 in total).
But the degree of which is not too large so we would not add regularisation which can shrink the polynomials too much so that the invariance may seem to be obeyed but it is just close to zero since the coefficients are small.
Also, we are only expanding up to the cubic term, therefore we do not expect very good learning for the invariance.
However, in our case, the approximation is fairly good, possibly due to relatively small value of $q, p$ so the polynomial approximation is good.
Nevertheless, it is clear that there is still some errors with learning invariance from figure \ref{fig:double_pendulum_energy} since it only flattens toward the end of the trajectory.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/double_pendulum_polynomial.pdf}
  \centering
  \caption{Double Pendulum learnt invariance}
  \label{fig:double_pendulum_poly}
\end{figure}
This time we found a Pearson corrletaiton of $-0.3644$ with a p-value of $0.0093$ between MSE and log marginal likelihood.
Combining with the fact that our optimised learnt kernel is better than the theortically approximately initialised kernel in both marginal likelihood and MSE, so we conclude that we have identified the correct invariance.  

\chapter{Conclusion and Future Work}
In this work, we have presented a new Gaussian Process model through an invariance kernel that is able to capture the invariance in a dynamical system and as a result, use it as an inductive bias to improve data efficiency and predictive performance in several one and two-dimensional physical systems. 
We construct this special invariance kernel by using the fact that invariances are linear transformation of the dynamics and the property of joint Gaussian distribution, and finally does the conditioning using simple Schur Complement calculation.
To learn this invariance condition, we parameterise it with polynomials and optimise the coefficients by marginal likelihood objective. 
The model is able to capture the correct invariance and recover the law of physics.
We have also demonstrated it is also robust against dissiaptive effects using a noise variable to allow "soft invariance" as well as using a latent variable to model the dissipative forces. 

In the future, there are several directions this project could evolve into.
To start off, we have demonstrated our results on relatively simple systems that could easily be sovled by first year physics undergraduate student. 
It would be interesting to see how our current model fits into the bigger picture of physical dynamical systems.
For instance, a charged particle in magnetic field still conserves energy with forces acting on it. 
Would our model able to accomdate that?
Or if the forces are time dependent, such as forced oscillation of pendulum, perhaps we need to include time as an input variable.
More generally, we are simply using the energy conservation to derive our kernel, and the form is more or less equivalent to Newton's law of motion.
There may be more complicated systems where this formulation is not sufficient.
For instance, the Lagrangian and Hamiltonian formula are much more general and could be even be extended to quantum mechanics.
For classical dynamical systems, if we want to model deformation of elastic materials or extended objects that are not point particles, this formulation may not be sufficient.
As a result, it is important to consult classical theoretical physics literature to help us understand the limitations of our models and then extend to form a more complete picture.  
In fact, Hamiltonian mechanics is the most natural way to incorporate invariance and symmetry as explained in section 2.3.
If we are able to formulate our model using Hamiltonian description, then we can easily include other symmetries such as momentum.
Another direction is to extend the partially observed system model.
We have successfully apply that in a special case of dissipative system.
However, as systems grow more complex, including a single latent dynamics may not be enough; instead, we may need to invent an latent input due to dissipative forces and derive its dyanmics by taking the time derivative.
As mentioned in section 3.4, an important and very common case is the missing input dimension problem.
If we are able to successfully recover the higher dimensional space by observing the lower dimensional space combine with the knowledge of invariance, it will be very useful in applications like 3D reconstruction.
More sophisticated inference technique will be needed such as Markov Chain Monte Carlo.


% References: modify the file refs.bib
\bibliographystyle{plainnat}
\bibliography{refs}


\clearpage
%% reset page counter and start appendix pages with A
\pagenumbering{arabic}
\renewcommand*{\thepage}{A\arabic{page}}
\appendix

\chapter{Appendix}

\section{Effect of Jitter}
Since we need a little bit of jitter to stabilise the computation of our invariance kernel as explained above, and that adding jitter is the same as adding noise to the invariance (since we are adding it on the diagonal of $LKL^T$ submatrix), we need to make sure it is not too large and too noisy, which will destroy the point of having invariance to start with. 
So we will again assess the performance in the similar way as above using different amount of jitter.
The results is shown in figure \ref{fig:vary_jitter}.

\begin{figure}[H] 
  \includegraphics[width=0.8\linewidth]{../codes/figures/vary_jitter.pdf}
  \centering
  \caption{Effect of varying jitter}
  \label{fig:vary_jitter}
\end{figure}

For marginal likelihood, we see it decreases as jitter increases as expected because then the invariance is not as strong a contraint.
For MSE, we expect to see steady decrease in performance.
However, here we see a dip initially.
Again, there is not a big difference, and that the lowest jitter might not be enough to stablise the model so that the worse performance could be due to numerical instability.
We see a similar pattern in energy conservation, where we see energy consevation is not as enforced when we have noisy invariance.
Degree of freedom increases with jitter as expected since we are relaxing the constraints.
While the lower the jitter, the higher the performance should be in theory; the instability of matrix inversion and numerical errors could cause a drop in performance.
As a result,  we will also choose the smallest amount of jitter that still gives stable performance.  
Note that, in both cases of varying jitter and invariance density; all of them are much better than the RBF in performance anyways.

\end{document}
