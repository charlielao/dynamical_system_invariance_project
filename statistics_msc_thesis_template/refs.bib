@book{alligood_sauer_yorke_2000, place={New-York}, title={Chaos: An introduction to dynamical systems}, publisher={Springer}, author={Alligood, Kathleen T. and Sauer, Tim and Yorke, James A.}, year={2000}} 
@ARTICLE{GPflow2017,
 author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
 title = "{ {GP}flow: A {G}aussian process library using {T}ensor{F}low}",
 journal = {Journal of Machine Learning Research},
 year = {2017},
 month = {apr},
 volume = {18},
 number = {40},
 pages = {1-6},
 url = {http://jmlr.org/papers/v18/16-537.html}
}
 @book{rasmussen_williams_2006, place={London, England}, title={Gaussian process for machine learning}, publisher={The MIT Press}, author={Rasmussen, Carl Edward and Williams, Christopher K I}, year={2006}} 
 @book{strogatz_2019, title={Nonlinear Dynamics and Chaos: With applications to physics, Biology, Chemistry and Engineering}, publisher={CRC Press}, author={Strogatz, Steven}, year={2019}} 
 @book{glendinning_1994, place={Cambridge}, title={Stability, instability and Chaos: An introduction to the theory of nonlinear differential equations}, publisher={Cambridge University Press}, author={Glendinning, Paul}, year={1994}} 
@book{marsden_sirovich_antman_2008, place={New York, NY}, title={Multiscale methods: Averaging and homogenization}, publisher={Springer New York}, author={Marsden, J. E. and Sirovich, L. and Antman, S. S.}, year={2008}} 
 @phdthesis{Kondor2008, title={Group theoretical methods in machine learning}, author={Kondor, Imre}, year={2008}, school={Columbia University}} 
 @book{lemos_2018, place={Cambridge}, title={Analytical Mechanics}, publisher={Cambridge University Press}, author={Lemos, Nivaldo A.}, year={2018}} 
 

@article{Papastamatiou2022,
   abstract = {A combined clustering/symbolic regression framework for fluid property prediction Physics of Fluids 34, 062004 (2022); https://doi.org/10.1063/5.0096669 Impact of inlet flow angle variation on the performance of a transonic compressor blade using NIPC AIP Advances 12, 025001 (2022); https://doi.org/10.1063/5.0074200 Broadband interconnected receiver-transmitter surface for generating dual circularly polarized dual beams AIP Advances 12, 025003 (2022); https://doi.org/10.1063/5.0075559 AIP Advances ARTICLE scitation.org/journal/adv ABSTRACT This work incorporates symbolic regression to propose simple and accurate expressions that fit to material datasets. The incorporation of symbolic regression in physical sciences opens the way to replace "black-box" machine learning techniques with representations that carry the physical meaning and can reveal the underlying mechanism in a purely data-driven approach. The application here is the extraction of analytical equations for the self-diffusion coefficient of the Lennard-Jones fluid by exploiting widely incorporating data from the literature. We propose symbolic formulas of low complexity and error that achieve better or comparable results to well-known microscopic and empirical expressions. Results refer to the material state space both as a whole and in distinct gas, liquid, and supercritical regions.},
   author = {Konstantinos Papastamatiou and Filippos Sofos and Theodoros E Karakasidis},
   doi = {10.1063/5.0082147},
   journal = {AIP Advances},
   pages = {25004},
   title = {Machine learning symbolic equations for diffusion with physics-based descriptions ARTICLES YOU MAY BE INTERESTED IN Machine learning symbolic equations for diffusion with physics-based descriptions},
   volume = {12},
   url = {http://creativecommons.org/licenses/by/4.0/},
   year = {2022},
}
@misc{flexiblePhysics,
  doi = {10.48550/ARXIV.1806.08047},
  
  url = {https://arxiv.org/abs/1806.08047},
  
  author = {Mrowca, Damian and Zhuang, Chengxu and Wang, Elias and Haber, Nick and Fei-Fei, Li and Tenenbaum, Joshua B. and Yamins, Daniel L. K.},
  
  keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flexible Neural Representation for Physics Prediction},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{NeuralODE,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Zhong2021,
   abstract = {The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.},
   author = {Yaofeng Desmond Zhong and Biswadip Dey and Amit Chakraborty},
   journal = {Proceedings of Machine Learning Research},
   keywords = {Deep Learning,Inductive Bias,Neural ODE,Physics-based Priors},
   pages = {1-12},
   title = {Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data},
   volume = {144},
   year = {2021},
}
@misc{Cranmer2020,
  doi = {10.48550/ARXIV.2003.04630},
  
  url = {https://arxiv.org/abs/2003.04630},
  
  author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
  
  keywords = {Machine Learning (cs.LG), Dynamical Systems (math.DS), Computational Physics (physics.comp-ph), Data Analysis, Statistics and Probability (physics.data-an), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Lagrangian Neural Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Cuomo2022, title={Scientific machine learning through physics–informed Neural Networks: Where we are and what’s next}, volume={92}, DOI={10.1007/s10915-022-01939-z}, number={3}, journal={Journal of Scientific Computing}, author={Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco}, year={2022}} 
@article{Jagtap2020,
   abstract = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.},
   author = {Ameya D. Jagtap and Ehsan Kharazmi and George Em Karniadakis},
   doi = {10.1016/J.CMA.2020.113028},
   issn = {00457825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Conservation laws,Domain decomposition,Inverse problems,Machine learning,Mortar PINN,cPINN},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
   volume = {365},
   year = {2020},
}
@article{Raissi2018,
   abstract = {While there is currently a lot of enthusiasm about “big data” useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
   author = {Maziar Raissi and George Em Karniadakis},
   doi = {10.1016/J.JCP.2017.11.039},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Bayesian modeling,Fractional equations,Probabilistic machine learning,Small data,System identification,Uncertainty quantification},
   month = {3},
   pages = {125-141},
   publisher = {Academic Press Inc.},
   title = {Hidden physics models: Machine learning of nonlinear partial differential equations},
   volume = {357},
   year = {2018},
}
@article{PIML,
   abstract = {0123456789();: Modelling and forecasting the dynamics of multiphysics and multiscale systems remains an open scientific problem. Take for instance the Earth system, a uniquely complex system whose dynamics are intricately governed by the interaction of physical, chemical and biological processes taking place on spatiotemporal scales that span 17 orders of magnitude 1. In the past 50 years, there has been tremendous progress in understanding multiscale physics in diverse applications, from geophysics to biophysics, by numerically solving partial differential equations (PDEs) using finite differences, finite elements, spectral and even meshless methods. Despite relentless progress, modelling and predicting the evolution of nonlinear multiscale systems with inhomogeneous cascades-of-scales by using classical analytical or computational tools inevitably faces severe challenges and introduces prohibitive cost and multiple sources of uncertainty. Moreover, solving inverse problems (for inferring material properties in functional materials or discovering missing physics in reactive transport, for example) is often prohibitively expensive and requires complex formulations, new algorithms and elaborate computer codes. Most importantly , solving real-life physical problems with missing, gappy or noisy boundary conditions through traditional approaches is currently impossible. This is where and why observational data play a crucial role. With the prospect of more than a trillion sensors in the next decade, including airborne, seaborne and satellite remote sensing, a wealth of multi-fidelity observations is ready to be explored through data-driven methods. However, despite the volume, velocity and variety of available (collected or generated) data streams, in many real cases it is still not possible to seamlessly incorporate such multi-fidelity data into existing physical models. Mathematical (and practical) data-assimilation efforts have been blossoming; yet the wealth and the spatiotem-poral heterogeneity of available data, along with the lack of universally acceptable models, underscores the need for a transformative approach. This is where machine learning (ML) has come into play. It can explore massive design spaces, identify multi-dimensional correlations and manage ill-posed problems. It can, for instance, help to detect climate extremes or statistically predict dynamic variables such as precipitation or vegetation productivity 2,3. Deep learning approaches, in particular, naturally provide tools for automatically extracting features from massive amounts of multi-fidelity observational data that are currently available and characterized by unprecedented spatial and temporal coverage 4. They can also help to link these features with existing approximate models and exploit them in building new predictive tools. Even for biophysical and biomedical modelling, this synergistic integration between ML tools and multiscale and multiphysics models has been recently advocated 5. Abstract | Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
   author = {George Em Karniadakis and Ioannis G Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
   doi = {10.1038/s42254-021-00314-5},
   isbn = {0123456789},
   journal = {Nature Reviews Physics},
   title = {Physics-informed machine learning},
   url = {www.nature.com/natrevphys},
   year={2021}
}
@article{Qian2020,
   abstract = {We present Lift \& Learn, a physics-informed method for learning low-dimensional models for large-scale dynamical systems. The method exploits knowledge of a system's governing equations to identify a coordinate transformation in which the system dynamics have quadratic structure. This transformation is called a lifting map because it often adds auxiliary variables to the system state. The lifting map is applied to data obtained by evaluating a model for the original nonlinear system. This lifted data is projected onto its leading principal components, and low-dimensional linear and quadratic matrix operators are fit to the lifted reduced data using a least-squares operator inference procedure. Analysis of our method shows that the Lift & Learn models are able to capture the system physics in the lifted coordinates at least as accurately as traditional intrusive model reduction approaches. This preservation of system physics makes the Lift & Learn models robust to changes in inputs. Numerical experiments on the FitzHugh-Nagumo neuron activation model and the compressible Euler equations demonstrate the generalizability of our model.},
   author = {Elizabeth Qian and Boris Kramer and Benjamin Peherstorfer and Karen Willcox},
   doi = {10.1016/J.PHYSD.2020.132401},
   issn = {01672789},
   journal = {Physica D: Nonlinear Phenomena},
   keywords = {Data-driven model reduction,Dynamical systems,Lifting map,Partial differential equations,Scientific machine learning},
   month = {5},
   publisher = {Elsevier B.V.},
   title = {Lift \& Learn: Physics-informed machine learning for large-scale nonlinear dynamical systems},
   volume = {406},
   year = {2020},
}
@article{Raissi2019,
   abstract = {We introduce physics-informed neural networks neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge-Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction-diffusion systems, and the propagation of nonlinear shallow-water waves.},
   author = {M. Raissi and P. Perdikaris and G. E. Karniadakis},
   doi = {10.1016/J.JCP.2018.10.045},
   issn = {0021-9991},
   journal = {Journal of Computational Physics},
   keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge Kutta methods},
   month = {2},
   pages = {686-707},
   publisher = {Academic Press},
   title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
   volume = {378},
   year = {2019},
}
@misc{GraphSymbolicPhysics,
  doi = {10.48550/ARXIV.1909.05862},
  
  url = {https://arxiv.org/abs/1909.05862},
  
  author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
  
  keywords = {Machine Learning (cs.LG), Instrumentation and Methods for Astrophysics (astro-ph.IM), Computational Physics (physics.comp-ph), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Learning Symbolic Physics with Graph Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Liu2021,
  title = {Machine Learning Conservation Laws from Trajectories},
  author = {Liu, Ziming and Tegmark, Max},
  journal = {Phys. Rev. Lett.},
  volume = {126},
  issue = {18},
  pages = {180604},
  numpages = {6},
  year = {2021},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.180604},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.180604}
}

@article{Udrescu2020,
   abstract = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: Finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.},
   author = {Silviu Marian Udrescu and Max Tegmark},
   doi = {10.1126/SCIADV.AAY2631/ASSET/DEA58C72-32B7-4207-BD40-91D257F0D857/ASSETS/GRAPHIC/AAY2631-F2.JPEG},
   issn = {23752548},
   issue = {16},
   journal = {Science Advances},
   month = {4},
   pmid = {32426452},
   publisher = {American Association for the Advancement of Science},
   title = {AI Feynman: A physics-inspired method for symbolic regression},
   volume = {6},
   url = {https://www.science.org/doi/10.1126/sciadv.aay2631},
   year = {2020},
}
@inproceedings{
Tycho2021,
title={Learning Invariant Weights in Neural Networks},
author={Tycho F.A. van der Ouderaa and Mark van der Wilk},
booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
year={2022},
url={https://openreview.net/forum?id=BVxfSPUoqeq}
}

@article{Baxter2000,
   abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
   author = {Jonathan Baxter},
   journal = {Journal of Artificial Intelligence Research},
   pages = {149-198},
   title = {A Model of Inductive Bias Learning},
   volume = {12},
   year = {2000},
}
@inproceedings{Mark2017,
 author = {Van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Invariances using the Marginal Likelihood},
 url = {https://proceedings.neurips.cc/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{DissipativeHNN,
  doi = {10.48550/ARXIV.2201.10085},
  
  url = {https://arxiv.org/abs/2201.10085},
  
  author = {Sosanya, Andrew and Greydanus, Sam},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dissipative Hamiltonian Neural Networks: Learning Dissipative and Conservative Dynamics Separately},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@InProceedings{Heinonen2018, title = 	 {Learning unknown {ODE} models with {G}aussian processes}, author =       {Heinonen, Markus and Yildiz, Cagatay and Mannerstr{\"o}m, Henrik and Intosalmi, Jukka and L{\"a}hdesm{\"a}ki, Harri}, booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning}, pages = 	 {1959--1968}, year = 	 {2018}, editor = 	 {Dy, Jennifer and Krause, Andreas}, volume = 	 {80}, series = 	 {Proceedings of Machine Learning Research}, month = 	 {10--15 Jul}, publisher =    {PMLR}, pdf = 	 {http://proceedings.mlr.press/v80/heinonen18a/heinonen18a.pdf}, url = 	 {https://proceedings.mlr.press/v80/heinonen18a.html}, abstract = 	 {In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.} }
@article{Rath2020,
author = {Rath,Katharina  and Albert,Christopher G.  and Bischl,Bernd  and von Toussaint,Udo },
title = {Symplectic Gaussian process regression of maps in Hamiltonian systems},
journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
volume = {31},
number = {5},
pages = {053121},
year = {2021},
doi = {10.1063/5.0048129},

URL = { 
        https://doi.org/10.1063/5.0048129
    
},
eprint = { 
        https://doi.org/10.1063/5.0048129
    
}

}
@inproceedings{HNN,
 author = {Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hamiltonian Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{Bronstein2017,
   abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
   author = {Michael M. Bronstein and Joan Bruna and Yann Lecun and Arthur Szlam and Pierre Vandergheynst},
   doi = {10.1109/MSP.2017.2693418},
   issn = {10535888},
   issue = {4},
   journal = {IEEE Signal Processing Magazine},
   month = {7},
   pages = {18-42},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Geometric Deep Learning: Going beyond Euclidean data},
   volume = {34},
   year = {2017},
}

