@book{alligood_sauer_yorke_2000, place={New-York}, title={Chaos: An introduction to dynamical systems}, publisher={Springer}, author={Alligood, Kathleen T. and Sauer, Tim and Yorke, James A.}, year={2000}} 
@ARTICLE{GPflow2017,
 author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
 title = "{ {GP}flow: A {G}aussian process library using {T}ensor{F}low}",
 journal = {Journal of Machine Learning Research},
 year = {2017},
 month = {apr},
 volume = {18},
 number = {40},
 pages = {1-6},
 url = {http://jmlr.org/papers/v18/16-537.html}
}
 @book{rasmussen_williams_2006, place={London, England}, title={Gaussian process for machine learning}, publisher={The MIT Press}, author={Rasmussen, Carl Edward and Williams, Christopher K I}, year={2006}} 
 @book{strogatz_2019, title={Nonlinear Dynamics and Chaos: With applications to physics, Biology, Chemistry and Engineering}, publisher={CRC Press}, author={Strogatz, Steven}, year={2019}} 
 @book{glendinning_1994, place={Cambridge}, title={Stability, instability and Chaos: An introduction to the theory of nonlinear differential equations}, publisher={Cambridge University Press}, author={Glendinning, Paul}, year={1994}} 
@book{marsden_sirovich_antman_2008, place={New York, NY}, title={Multiscale methods: Averaging and homogenization}, publisher={Springer New York}, author={Marsden, J. E. and Sirovich, L. and Antman, S. S.}, year={2008}} 
@article{Kondor2008,
   author = {Risi Kondor},
   title = {Group theoretical methods in machine learning},
   year = {2008},
}
 @book{lemos_2018, place={Cambridge}, title={Analytical Mechanics}, publisher={Cambridge University Press}, author={Lemos, Nivaldo A.}, year={2018}} 
 

@article{Papastamatiou2022,
   abstract = {A combined clustering/symbolic regression framework for fluid property prediction Physics of Fluids 34, 062004 (2022); https://doi.org/10.1063/5.0096669 Impact of inlet flow angle variation on the performance of a transonic compressor blade using NIPC AIP Advances 12, 025001 (2022); https://doi.org/10.1063/5.0074200 Broadband interconnected receiver-transmitter surface for generating dual circularly polarized dual beams AIP Advances 12, 025003 (2022); https://doi.org/10.1063/5.0075559 AIP Advances ARTICLE scitation.org/journal/adv ABSTRACT This work incorporates symbolic regression to propose simple and accurate expressions that fit to material datasets. The incorporation of symbolic regression in physical sciences opens the way to replace "black-box" machine learning techniques with representations that carry the physical meaning and can reveal the underlying mechanism in a purely data-driven approach. The application here is the extraction of analytical equations for the self-diffusion coefficient of the Lennard-Jones fluid by exploiting widely incorporating data from the literature. We propose symbolic formulas of low complexity and error that achieve better or comparable results to well-known microscopic and empirical expressions. Results refer to the material state space both as a whole and in distinct gas, liquid, and supercritical regions.},
   author = {Konstantinos Papastamatiou and Filippos Sofos and Theodoros E Karakasidis},
   doi = {10.1063/5.0082147},
   journal = {AIP Advances},
   pages = {25004},
   title = {Machine learning symbolic equations for diffusion with physics-based descriptions ARTICLES YOU MAY BE INTERESTED IN Machine learning symbolic equations for diffusion with physics-based descriptions},
   volume = {12},
   url = {http://creativecommons.org/licenses/by/4.0/},
   year = {2022},
}
@article{flexiblePhysics,
   abstract = {Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail. Inspired by this ability, we propose a hierarchical particle-based object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials. We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations. These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.},
   author = {Damian Mrowca and Chengxu Zhuang and Elias Wang and Nick Haber and Li Fei-Fei and Joshua B Tenenbaum and Daniel L K Yamins},
   keywords = {1,Electrical Engineering 3,Pediatrics 4 and Biomedical Data Science 5,Psychology 2,and Wu Tsai Neurosciences Institute 6},
   title = {Flexible Neural Representation for Physics Prediction},
}
@article{NeuralODE,
   abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
   author = {Ricky T Q Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
   title = {Neural Ordinary Differential Equations},
}
@article{Zhong2021,
   abstract = {The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.},
   author = {Yaofeng Desmond Zhong and Biswadip Dey and Amit Chakraborty},
   journal = {Proceedings of Machine Learning Research},
   keywords = {Deep Learning,Inductive Bias,Neural ODE,Physics-based Priors},
   pages = {1-12},
   title = {Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data},
   volume = {144},
   year = {2021},
}
@article{Cranmer2020,
   abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation. Figure 1: Physicists use Lagrangians to describe the dynamics of physical systems like the double pendulum (black). Neural networks struggle to model these dynamics over long time periods due to their inability to conserve energy (red). In this paper, we show how to learn arbitrary Lagrangians with neural networks, inducing a strong physical prior on the learned dynamics (blue). * Also affiliated with Princeton University},
   author = {Miles Cranmer and Sam Greydanus and Stephan Hoyer and Google Research and Peter Battaglia and David Spergel and Shirley Ho},
   title = {LAGRANGIAN NEURAL NETWORKS},
   year = {2020},
}

@article{Cuomo2022,
   abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to 2 Scientific Machine Learning through PINNs characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
   author = {Salvatore Cuomo and Vincenzo Schiano and Di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli},
   keywords = {Deep Neural Networks,Nonlinear equations,Numerical methods,Partial Differential Equations,Physics-Informed Neural Networks,Scientific Machine Learning,Uncertainty},
   title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
   year = {2022},
}
@article{Jagtap2020,
   abstract = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.},
   author = {Ameya D. Jagtap and Ehsan Kharazmi and George Em Karniadakis},
   doi = {10.1016/J.CMA.2020.113028},
   issn = {00457825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Conservation laws,Domain decomposition,Inverse problems,Machine learning,Mortar PINN,cPINN},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
   volume = {365},
   year = {2020},
}
@article{Raissi2018,
   abstract = {While there is currently a lot of enthusiasm about “big data” useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
   author = {Maziar Raissi and George Em Karniadakis},
   doi = {10.1016/J.JCP.2017.11.039},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Bayesian modeling,Fractional equations,Probabilistic machine learning,Small data,System identification,Uncertainty quantification},
   month = {3},
   pages = {125-141},
   publisher = {Academic Press Inc.},
   title = {Hidden physics models: Machine learning of nonlinear partial differential equations},
   volume = {357},
   year = {2018},
}
@article{PIML,
   abstract = {0123456789();: Modelling and forecasting the dynamics of multiphysics and multiscale systems remains an open scientific problem. Take for instance the Earth system, a uniquely complex system whose dynamics are intricately governed by the interaction of physical, chemical and biological processes taking place on spatiotemporal scales that span 17 orders of magnitude 1. In the past 50 years, there has been tremendous progress in understanding multiscale physics in diverse applications, from geophysics to biophysics, by numerically solving partial differential equations (PDEs) using finite differences, finite elements, spectral and even meshless methods. Despite relentless progress, modelling and predicting the evolution of nonlinear multiscale systems with inhomogeneous cascades-of-scales by using classical analytical or computational tools inevitably faces severe challenges and introduces prohibitive cost and multiple sources of uncertainty. Moreover, solving inverse problems (for inferring material properties in functional materials or discovering missing physics in reactive transport, for example) is often prohibitively expensive and requires complex formulations, new algorithms and elaborate computer codes. Most importantly , solving real-life physical problems with missing, gappy or noisy boundary conditions through traditional approaches is currently impossible. This is where and why observational data play a crucial role. With the prospect of more than a trillion sensors in the next decade, including airborne, seaborne and satellite remote sensing, a wealth of multi-fidelity observations is ready to be explored through data-driven methods. However, despite the volume, velocity and variety of available (collected or generated) data streams, in many real cases it is still not possible to seamlessly incorporate such multi-fidelity data into existing physical models. Mathematical (and practical) data-assimilation efforts have been blossoming; yet the wealth and the spatiotem-poral heterogeneity of available data, along with the lack of universally acceptable models, underscores the need for a transformative approach. This is where machine learning (ML) has come into play. It can explore massive design spaces, identify multi-dimensional correlations and manage ill-posed problems. It can, for instance, help to detect climate extremes or statistically predict dynamic variables such as precipitation or vegetation productivity 2,3. Deep learning approaches, in particular, naturally provide tools for automatically extracting features from massive amounts of multi-fidelity observational data that are currently available and characterized by unprecedented spatial and temporal coverage 4. They can also help to link these features with existing approximate models and exploit them in building new predictive tools. Even for biophysical and biomedical modelling, this synergistic integration between ML tools and multiscale and multiphysics models has been recently advocated 5. Abstract | Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
   author = {George Em Karniadakis and Ioannis G Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
   doi = {10.1038/s42254-021-00314-5},
   isbn = {0123456789},
   journal = {Nature Reviews Physics},
   title = {Physics-informed machine learning},
   url = {www.nature.com/natrevphys},
}
@article{Qian2020,
   abstract = {We present Lift \& Learn, a physics-informed method for learning low-dimensional models for large-scale dynamical systems. The method exploits knowledge of a system's governing equations to identify a coordinate transformation in which the system dynamics have quadratic structure. This transformation is called a lifting map because it often adds auxiliary variables to the system state. The lifting map is applied to data obtained by evaluating a model for the original nonlinear system. This lifted data is projected onto its leading principal components, and low-dimensional linear and quadratic matrix operators are fit to the lifted reduced data using a least-squares operator inference procedure. Analysis of our method shows that the Lift & Learn models are able to capture the system physics in the lifted coordinates at least as accurately as traditional intrusive model reduction approaches. This preservation of system physics makes the Lift & Learn models robust to changes in inputs. Numerical experiments on the FitzHugh-Nagumo neuron activation model and the compressible Euler equations demonstrate the generalizability of our model.},
   author = {Elizabeth Qian and Boris Kramer and Benjamin Peherstorfer and Karen Willcox},
   doi = {10.1016/J.PHYSD.2020.132401},
   issn = {01672789},
   journal = {Physica D: Nonlinear Phenomena},
   keywords = {Data-driven model reduction,Dynamical systems,Lifting map,Partial differential equations,Scientific machine learning},
   month = {5},
   publisher = {Elsevier B.V.},
   title = {Lift \& Learn: Physics-informed machine learning for large-scale nonlinear dynamical systems},
   volume = {406},
   year = {2020},
}
@article{Raissi2019,
   abstract = {We introduce physics-informed neural networks neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge-Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction-diffusion systems, and the propagation of nonlinear shallow-water waves.},
   author = {M. Raissi and P. Perdikaris and G. E. Karniadakis},
   doi = {10.1016/J.JCP.2018.10.045},
   issn = {0021-9991},
   journal = {Journal of Computational Physics},
   keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge Kutta methods},
   month = {2},
   pages = {686-707},
   publisher = {Academic Press},
   title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
   volume = {378},
   year = {2019},
}
@article{GraphSymbolicPhysics,
   abstract = {We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.},
   author = {Miles D Cranmer and Rui Xu and Peter Battaglia and Shirley Ho},
   title = {Learning Symbolic Physics with Graph Networks},
}
@article{Liu2021,
   abstract = {We present AI Poincaré, a machine learning algorithm for autodiscovering conserved quantities using trajectory data from unknown dynamical systems. We test it on five Hamiltonian systems, including the gravitational three-body problem, and find that it discovers not only all exactly conserved quantities, but also periodic orbits, phase transitions, and breakdown timescales for approximate conservation laws.},
   author = {Ziming Liu and Max Tegmark},
   doi = {10.1103/PhysRevLett.126.180604},
   keywords = {doi:10.1103/PhysRevLett.126.180604 url:https://doi.org/10.1103/PhysRevLett.126.180604},
   title = {Machine Learning Conservation Laws from Trajectories},
   year = {2021},
}
@article{Udrescu2020,
   abstract = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: Finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.},
   author = {Silviu Marian Udrescu and Max Tegmark},
   doi = {10.1126/SCIADV.AAY2631/ASSET/DEA58C72-32B7-4207-BD40-91D257F0D857/ASSETS/GRAPHIC/AAY2631-F2.JPEG},
   issn = {23752548},
   issue = {16},
   journal = {Science Advances},
   month = {4},
   pmid = {32426452},
   publisher = {American Association for the Advancement of Science},
   title = {AI Feynman: A physics-inspired method for symbolic regression},
   volume = {6},
   url = {https://www.science.org/doi/10.1126/sciadv.aay2631},
   year = {2020},
}
@article{Tycho2021,
   abstract = {It is well-known that assumptions about invari-ances or symmetries in data can seriously increase the predictive power of statistical models. Many commonly used models in machine learning have certain symmetries built-in, such as translation equivariance in convolutional neural networks, and incorporation of new symmetry types are actively being studied. Yet, efforts to learn such invariances from the data itself are in their early days and inferring invariances remains an open research problem. It has been shown that marginal likelihood offers a principled way to learn in-variances in Gaussian Processes. We propose a weight-space equivalent to this approach, by minimizing a lower bound on the marginal likelihood to learn invariances in neural networks.},
   author = {Tycho FA van der Ouderaa and Mark van der Wilk},
   title = {Learning Invariant Weights in Neural Networks},
   year = {2021},
}

@article{Baxter2000,
   abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
   author = {Jonathan Baxter},
   journal = {Journal of Artificial Intelligence Research},
   pages = {149-198},
   title = {A Model of Inductive Bias Learning},
   volume = {12},
   year = {2000},
}

@article{Mark2017,
   abstract = {Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations of the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation , where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by backpropagating through it to maximise the marginal likelihood.},
   author = {Mark Van Der Wilk and Matthias Bauer and S T John and James Hensman},
   title = {Learning Invariances using the Marginal Likelihood},
}
@article{DissipativeHNN,
   abstract = {Understanding natural symmetries is key to making sense of our complex and ever-changing world. Recent work has shown that neural networks can learn such symmetries directly from data using Hamiltonian Neural Networks (HNNs). But HNNs struggle when trained on datasets where energy is not conserved. In this paper, we ask whether it is possible to identify and decompose conservative and dissipative dynamics simultaneously. We propose Dissipative Hamil-tonian Neural Networks (D-HNNs), which pa-rameterize both a Hamiltonian and a Rayleigh dissipation function. Taken together, they represent an implicit Helmholtz decomposition which can separate dissipative effects such as friction from symmetries such as conservation of energy. We train our model to decompose a damped mass-spring system into its friction and inertial terms and then show that this decomposition can be used to predict dynamics for unseen friction coefficients. Then we apply our model to real world data including a large, noisy ocean current dataset where decomposing the velocity field yields useful scientific insights.},
   author = {Andrew Sosanya and Sam Greydanus},
   title = {Dissipative Hamiltonian Neural Networks: Learning Dissipative and Conservative Dynamics Separately},
}
@article{Heinonen2018,
   abstract = {In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of non-parametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model's capabilities to infer dynamics from sparse data and to simulate the system forward into future.},
   author = {Markus Heinonen and C ¸ Agatay Yıldız and Henrik Mannerström and Jukka Intosalmi and Harri Lähdesmäki},
   title = {Learning unknown ODE models with Gaussian processes},
   url = {http://www.},
   year = {2018},
}
@article{Rath2020,
   abstract = {We present an approach to construct appropriate and efficient emulators for Hamiltonian flow maps. Intended future applications are long-term tracing of fast charged particles in accelerators and magnetic plasma confinement configurations. The method is based on multi-output Gaussian process regression on scattered training data. To obtain long-term stability the symplectic property is enforced via the choice of the matrix-valued covari-ance function. Based on earlier work on spline interpolation we observe derivatives of the generating function of a canonical transformation. A product kernel produces an accurate implicit method, whereas a sum kernel results in a fast explicit method from this approach. Both correspond to a symplectic Euler method in terms of numerical integration. These methods are applied to the pendulum and the Hénon-Heiles system and results compared to an symmetric regression with orthogonal polynomials. In the limit of small mapping times, the Hamiltonian function can be identified with a part of the generating function and thereby learned from observed time-series data of the system's evolution. Besides comparable performance of implicit kernel and spectral regression for symplectic maps, we demonstrate a substantial increase in performance for learning the Hamiltonian function compared to existing approaches.},
   author = {Katharina Rath and Christopher G Albert and Bernd Bischl and Udo Von Toussaint},
   isbn = {2009.05569v1},
   title = {Symplectic Gaussian Process Regression of Hamiltonian Flow Maps Symplectic Gaussian Process Regression of Hamiltonian Flow Maps Symplectic Gaussian Process Regression of Hamiltonian Flow Maps},
   year = {2020},
}
@article{HNN,
   abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time. Ideal mass-spring system Noisy observations Baseline NN Prediction Prediction Hamiltonian NN Figure 1: Learning the Hamiltonian of a mass-spring system. The variables q and p correspond to position and momentum coordinates. As there is no friction, the baseline's inner spiral is due to model errors. By comparison, the Hamiltonian Neural Network learns to exactly conserve a quantity that is analogous to total energy.},
   author = {Sam Greydanus Google Brain and Misko Dzamba PetCube and Jason Yosinski},
   title = {Hamiltonian Neural Networks},
}

@article{Bronstein2017,
   abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
   author = {Michael M. Bronstein and Joan Bruna and Yann Lecun and Arthur Szlam and Pierre Vandergheynst},
   doi = {10.1109/MSP.2017.2693418},
   issn = {10535888},
   issue = {4},
   journal = {IEEE Signal Processing Magazine},
   month = {7},
   pages = {18-42},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Geometric Deep Learning: Going beyond Euclidean data},
   volume = {34},
   year = {2017},
}

