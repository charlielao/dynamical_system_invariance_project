\documentclass{article}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{bbm}
\usepackage[most]{tcolorbox}
\newcommand{\e}[1]{{\mathbb E}\left[ #1 \right]}
\definecolor{block-gray}{gray}{0.90}
\newtcolorbox{code}{colback=block-gray,grow to right by=-1mm,grow to left by=-1mm,boxrule=0pt,boxsep=0pt,breakable}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\title{\vspace{-3cm}Learning invariances in Dynamical System Problem Statement\vspace{-3em}}
\author{}
\date{}

\begin{document}
\maketitle
The aim of the project is to learn the invariances of a dynamical system and as a result predict the future observations by imposing physical inductive bias.  
Say we have a simply simple harmonic oscillator with the defining equation $m\ddot{x} + kx = 0.$
Then we will have the equation of motion $x(t) = c_1cos(\omega t) +c_2sin(\omega t)$, where the constants depend on the initial condition. 
Obviously, this is a very simple system, that does not need the any machine learning. 
However, as we have a chaotic system, like a double pendulum, we ran into the problem of high sensitivity to the initial condition, especially in high energy condition. 

Given data, we can use traditional time series approach to try to fit the model; however, as the system gets complex, it is no longer scalable. 

Our task would essentially be given the dataset $\mathcal{D}$, which consists of observations $p_{1,0}, p_{1,1}, \dots, p_{1,T}, q_{1,0}, \dots, q_{1,T}, p_{2,0}, \dots, p_{2,T}, q_{2,0}, \dots, q_{2,T}, \dots, p_{n,0}, \dots, p_{n,T}, q_{n,0}, \dots, q_{n,T}$, where p is canoical momentum and q is the canocial coordinates, with T being the time steps we observed, and n is the number of degree of freedom in the system, 
we will then try to predict the $q_{i,t}, p_{i,t}$ for $t>T$ as far into the future as possible, essentially future trajectory. 

There are essentially two routes: given dynamics (Hamiltonian) or not. 
If given, then we can do exactly what was done in the learning invariance paper, using Lie series etc. (uses conservation of everything)
If it is not given, perhaps we can use the HNN and convert it to GP, (uses Hamiltonian equations)
or use Liouville theorem. (uses conservation of energy)

A particular good therom of physics to apply in this context is Liouville Theorem, which is commonly used in statistical physics. 
The equation is given by 
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial t}+\sum_{i=1}^{n}\left(\frac{\partial \rho}{\partial q_{i}} \dot{q}_{i}+\frac{\partial \rho}{\partial p_{i}} \dot{p}_{i}\right)=0
$$
Here the $\rho$ is the particle phase density (hence commonly used in statistical physics), if we only have finite number of particles in a more standard set up like pendulums, then we can encode uncertainty (noise) into this phase space volume. 
This theorm is essentially telling us that the phase space volume is conserved throughout the evolution of the system given the energy (Hamiltonian) is conserved. 
The idea would be first imposing a GP on the phase density $\rho$, as a function of phases $p, q$, so that $\rho(p, q)\sim \mathcal{GP}(0, K)$, where K is the covariance function kernel (what would we use for kernel).
We then know that the total derivative of $\rho$ with respect to time $t$ is zero (we assume no explicit dependence on $t$ such that $\frac{\partial \rho}{\partial t}=0.$)
Without information about the dynamics, we cannot get an expression for the total derivative (since it involves Hamiltonian), we therefore approximate it with finite difference $\frac{\rho(p(t+1), q(t+1))-\rho(p(t), q(t))}{t}$.  
If we let this finite difference be the linear operator $L$, we then obtain $L[\rho]=0$.
Since GP stays GP under linear operations, we have $L[\rho]\sim GP(0, L^TKL)$.
At finite number of test points, $\rho_*$, we have $\rho_*\sim \mathcal{N}(0, K)$. 
Then we can condition on $L[\rho_*]\sim\mathcal{N}(0, L^TKL)=0$.
The results should be directly calculable (analytically).
We will then tune according to marginal likelihood.
Esseentially, we will have that for all time steps t, $L{\rho}=0.$
Therefore, overall hopefully, without too much data, we will able to learn essentially the phase space trajectory.

The other approach would be closer to using the real invariance of the system.
We can view the evolution of the system in phase space as a canonical transform generated by a generator function, $G$.
If the Possion bracket $\{\cdot,\mathcal{H}\}=0$, where 
$$
\{F, G\}=\sum_{k=1}^{n}\left(\frac{\partial F}{\partial q_{k}} \frac{\partial G}{\partial p_{k}}-\frac{\partial F}{\partial p_{k}} \frac{\partial G}{\partial q_{k}}\right)
$$, 
then $G$ is conserved.  
For instance, the generator of translation motion is $P$, and if the linear momentum is conserved (system is invariance under translation), then $\{P, \mathcal{H}\}=0$.
Naturally, $\{\mathcal{H}, \mathcal{H}\}$=0, so energy is conserved. 
This is the Hamiltonian formulation of Noether Theorem.
Now, we can actually express the evolution of state using a Lie series
$$
u(t)=u(0)+t\{u, H\}_{0}+\frac{t^{2}}{2 !}\{\{u, H\}, H\}_{0}+\frac{t^{3}}{3 !}\{\{\{u, H\}, H\}, H\}_{0}+\cdots .
$$
and therefore, we can write it as 
$$
u(t)=e^{t \mathcal{L}_{H}} u(0)
$$, 
which is called the time evolution operator. 
This can be readily implemented as the study with learning invariance weights paper (where the evolution generator is rotation instead)
However, this is given we get hold of the form of Hamiltonian, which I do not think is given out too much information since even a three body problem or a double pendulum have very simple dynamics but very difficult to predict. 
Perhaps we can also approximate the Possion Bracket, as $\delta u = \epsilon\{u, G\}$ so we have $u' = u + \epsilon\{u, G\}$, where $G$ is the appropriate generator.
If we replaced $\mathcal{H}$, we might have an approximation for the Poisson bracket. 
We can also write $\{\mathcal{H}, u\} = \frac{du}{dt}$.
If we have two Poisson bracket, it can be approximated by the second derivative etc., so we actually don't need the Hamiltonian.  
Perhaps we can express the coordinates in a basis (like Laplace or something) and then write differential operator as a matrix operator, then we can proceed as in the paper.  
Here the energy is conserved (under transformation in time).
So we will be essentially learning energy landscape.
The data has invariance in energy under time evolution just as digits has invariance in classification under rotation.

In Hamiltonian Neural Network, they actually did not use any symmetry or invariance properties, instead they simply use the Hamiltonian equation of motions. 
We could probably also construct a GP version of this using linear differentiation operator. 
We then might be able to impose the invaraince in energy in the weights under time translation since we can get hold of the Hamiltonian without knowing the dynamics.





Conservation of energy is symmetry in time.
Conservation of roational momentum is symmetry in rotation.
Time dependent kernel.
It will be cool, if we can feed in unlabelled trajectory data (does not even have label), and automatically predict the new ones.

What are we allowed to tell the system?
Its Hamiltonian?
Its time derivative?
Its symmetry?
How does knowing the invariance help us predict anything?
Does the fact that we know energy is conserved help us predict the motion?
convert Hamiltonian NN to GP?
Hamiltonian NN works because of Hamiltonian equations, not symmetry or invariances
Symmetry in time is a lot more abstract.
How to impose invariances on the trajectory.
Translation in time?
Translation in canonical coordinates?
Learn an energy surface with GP then do HNN?
Cannot paramertise the symmetry easily.
Lie series.
Canoical transform, lots of conserved quantities.



\section*{Reference}

\end{document}

